\documentclass[xcolor=dvipsnames]{beamer}
\usecolortheme[named=OliveGreen]{structure}
\usetheme[height=7mm]{Rochester}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{tikz}
\usetikzlibrary{arrows,petri,positioning,shapes}

\setbeamertemplate{items}[ball]
\setbeamertemplate{blocks}[rounded][shadow=true]
\setbeamercovered{invisible}
\setbeamertemplate{navigation symbols}{} % Remove navigation symbols
\setbeamertemplate{headline}{}
\setlength{\parskip}{1em}



\title{Regularized Laplacian Estimation and \\ Fast Eigenvector Approximation}
\author{Patrick~O.~Perry \and Michael~W.~Mahoney}
\institute[NYU Stern and Stanford] {
  NYU Stern \and Stanford University
}
\date{\today}

\begin{document}
% For every picture that defines or uses external nodes, you'll have to
% apply the 'remember picture' style. To avoid some typing, we'll apply
% the style to all pictures.
\tikzstyle{every picture}+=[remember picture]

% By default all math in TikZ nodes are set in inline mode. Change this to
% displaystyle so that we don't get small fractions.
\everymath{\displaystyle}

\begin{frame}
  \titlepage
\end{frame}


\begin{frame}[c]
  \begin{block}{}
  \begin{center}
    \huge{Introduction}
  \end{center}
  \end{block}
\end{frame}


\begin{frame}
  \begin{block}{Eigenvectors reveal structure}
  \begin{description}
    \item[Modularity matrix] community detection
    \item[Graph Laplacian] spectral clustering
    \item[Covariance matrix] factor analysis
  \end{description}
  \end{block}
  
  One way to visualize and simplify high-dimensional data is to form a matrix
  capturing structural information about the dataset and then to focus on
  the leading eigenvectors of this matrix.
\end{frame}


\begin{frame}
  \begin{block}{Diffusions reveal structure}
  \begin{description}
    \item[Heat diffusion] evolution determined by heat equation
    \item[PageRank surfer] random walk with teleportation
    \item[Truncated random walk] finite-length random walk
  \end{description}
  \end{block}

  Alternatively, one can distributed random charge on the data points and
  let the distribution of the charge according to a diffusion; the properties
  of this diffusion are related to the geometry of the dataset.

  Mahoney~and~Orecchia~\cite{MO11-implementing} showed that diffusions arise
  as solutions to \emph{regularized} eigenvector problems.
\end{frame}


\begin{frame}
  \begin{block}{Regularization}
  \begin{description}
    \item[Original problem] minimize $f(x)$
    \item[Regularized problem] minimize $f(x) + \frac{1}{\eta} g(x)$
  \end{description}
  \end{block}

  Regularization is a general principle where we add a penalty function to
  the optimization criterion.  E.g., $g(x) = \|x\|_2$ or $g(x) = \|x\|_1$.
  The solution to the regularized problem is
  often less sensitive to perturbations of the input data, and thus is
  has better statistical robustness.
\end{frame}

\begin{frame}
\centering
{\Large Big picture}
\vspace{-4em}

\begin{tikzpicture}[scale=2]
    \matrix[nodes={draw, ultra thick, fill=blue!20, minimum width=6em, minimum height=4em},
        row sep=1em,column sep=3em,ampersand replacement=\&] {
      \node[ellipse] (graphs) {Graphs};\&
      \node[ellipse] (regularization) {Regularization};\&
      \node[ellipse] (structure) {$\stackrel{\hbox{Diffusion/}}{\hbox{Structure}}$};\\
    };
    \path[<->, ultra thick]
      (regularization)
        edge node[auto, rotate=45, anchor=south west] {Mahoney \& Orecchia}
          (structure)
      (graphs)
        edge [bend right=30] node[below] {Empirical experience}
          (structure);
    \path[<->, ultra thick, dashed]
      (graphs)
        edge node[above, rotate=45, anchor=south west] {This work}
          (regularization);
\end{tikzpicture}
From empirical experience, we know that certain diffusions are more
appropriate for certain classes of graphs.
From Mahoney and Orecchia, we know certain diffusions arise as solutions
to certain regularized eigenvector problems.
The current work attempts to complete the circle by showing that certain 
type of regularization make certain implicit assumptions about structural
properties of the graph.
\end{frame}


\begin{frame}[c]
  \begin{block}{}
  \begin{center}
    \huge{Background}
  \end{center}
  \end{block}
\end{frame}


\begin{frame}
  \begin{block}{Notation for a weighted undirected graph}
  \begin{itemize}
    \item vertex set $V = \{ 1, \dotsc, n \}$ 
    \item edge set $E \subset V \times V$
    \item edge weight function $w : E \to \mathbb{R}^+$
    \item degree function $d : V \to \mathbb{R}^+$, $d(u) = \sum_v w(u,v)$
    \item diagonal degree matrix $D \in \mathbb{R}^\times{V \times V}$, $D(v,v) = d(v)$
    \item combinatorial Laplacian $L_0 \in \mathbb{R}^{V \times V}$,
    	$L_0(u,v) = \begin{cases}
		-w(u,v) &\text{when $u \neq v$} \\
		d(u) - w(u,v) &\text{otherwise.}
	\end{cases}$
    \item normalized Laplacian $L = D^{-1/2} \, L_0 \, D^{-1/2}$
  \end{itemize}
  \end{block}
  We will need the above notation.
\end{frame}

\begin{frame}
  \begin{block}{Diffusion-based procedures}
  \begin{description}
    \item[Heat Kernel] Charge evolves according to the heat equation
    	$\frac{\partial H_t}{\partial t} = - L H_t$.
    \item[PageRank] Charge at a node evlolves by either moving to a neighbor
    	of the current node or teleporting to a random node.
    \item[Lazy Random Walk] Charge either stays at the current node or moves
        to a neighbor.
  \end{description}
  \end{block}
  In each of these diffusions, we distributed charge on the vertices of a
  graph, and let the charge evolve according to certain dynamics.  The
  specific choice of the dynamics (Heat Kernel, PageRank, or Lazy Random Walk)
  is determined by structural features of the graph.  To date, there is some
  empirical understanding of which dynamics are appropriate for which classes
  of graphs, but no theoretical basis for these differences.
\end{frame}

\begin{frame}
  \begin{block}{Original eigenvector problem}
  \begin{columns}
  \column{0.3\textwidth}
    \begin{equation*}
    \begin{aligned}
    & \underset{x}{\text{minimize}}
    & & x^\mathrm{T} L x \\
    & \text{subject to}
    & & \| x \|_2 = 1, \\
    & & & x^\mathrm{T} D^{1/2} 1 = 0 \\
    & & &
    \end{aligned}
    \end{equation*}
    (standard form)
  \column{0.3\textwidth}
    \begin{equation*}
    \begin{aligned}
    & \underset{X}{\text{minimize}}
    & & \mathrm{Tr}(L X)\\
    & \text{subject to}
    & & \mathrm{Tr}(X) = 1, \\
    & & & X D^{1/2} 1 = 0, \\
    & & & X \succeq 0
    \end{aligned}
    \end{equation*}
    (relaxed form)
  \end{columns}
  \end{block}
  The correspondence between the two is that the solutions satisfy
  $x x^\mathrm{T} = X$.
\end{frame}

\begin{frame}
  \begin{block}{Regularized eigenvector problem}
    \begin{equation*}
    \begin{aligned}
    & \underset{X}{\text{minimize}}
    & & \mathrm{Tr}(L X) + \tfrac{1}{\eta} G(X) \\
    & \text{subject to}
    & & \mathrm{Tr}(X) = 1, \\
    & & & X D^{1/2} 1 = 0 \\
    & & & X \succeq 0
    \end{aligned}
    \end{equation*}
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{Mahoney \& Orecchia's result}
    \centering
    \begin{tabular}{ccc}
      Penalty &\phantom{MMM} & Solution \\
      \hline
      $\mathrm{Tr}(X \log X) - \mathrm{Tr}(X)$ && Heat Kernel \\
      $- \log |X|$ && PageRank  \\
      $\mathrm{Tr}(X^k)$ && Lazy Random Walk
    \end{tabular}
  \end{block}
  Mahoney \& Orecchia's result shows that certain choices of the penalty
  function $G(X)$ result in the solutions of the regularized eigenvector
  problem involving certain diffusions (summarized by the table).
\end{frame}


\begin{frame}
  \begin{block}{}
  \begin{center}
    \huge{Statistical framework for regularized graph estimation}
  \end{center}
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{Analogy with regularized linear regression}
  \begin{itemize}
    \item Observe $n$ predictor-response pairs in $\mathbb{R}^p \times \mathbb{R}$:
      $(x_1, y_1), \dotsc, (x_n, y_n)$
    \item Original problem: find $\beta$ such that $\beta' x_i \approx y_i$; minimize $F(\beta) =
    \sum_i \|y_i - \beta' x_i \|_2^2$
    \item Regularized problem: minimize $F(\beta) + \lambda \| \beta \|_2^2$
    (ridge) or minimize $F(\beta) + \lambda \| \beta \|_1$ (lasso)
  \end{itemize}
  \end{block}
\end{frame}

\begin{frame}
  \begin{tikzpicture}
    [pre/.style={<-, >=stealth', semithick},
     post/.style={->, >=stealth', semithick}]

    \node (model) {$y_i | x_i, \beta \sim \mathrm{Normal}(x_i' \beta, \sigma^2)$};

    \node (normal)  [below left=of model, xshift=2.5cm]  {$\beta_j \sim \mathrm{Normal}(0, \tau^2)$};
    \node (laplace) [below right=of model, xshift=-2.5cm] {$\beta_j \sim \mathrm{Laplace}(\mu)$};
    \node (tee) [circle, at = (model |- normal), yshift=1cm, inner sep=0, fill=black, minimum size=0.5pt, draw] {}
      edge[pre] (model)
      edge[post] (normal)
      edge[post] (laplace);

    \node (l2) [below=of normal]  {$\ell_2$-regularized LS}
      edge[pre] (normal);
    \node (l1) [below=of laplace] {$\ell_1$-regularized LS}
      edge[pre] (laplace);

    \node (prior label) [left=of normal]                       {Prior};
    \node (model label) [at=(prior label |- model)]            {Model};
    \node (map label)   [at=(prior label |- l2), align=center] {MAP\\ Estimate};
  \end{tikzpicture}

  Regularization is equivalent to ``Bayesianization'' in the following sense:
  the solution to the regularized problem is equal to the maximim \textit{a posteriori} probability
  (MAP) estimate of the parameter with a prior determined by the
  regularization penalty.
\end{frame}

\begin{frame}
\begin{figure}[h]
    \centering
    \subfigure[Dirichlet distribution order statistics.]{
      \makebox{\label{F:dirichlet}\includegraphics[scale=0.5]{plots/dirichlet}}
    }
    \subfigure[Spectrum of the inverse Laplacian.]{
      \makebox{\label{F:interpolate}\includegraphics[scale=0.5]{plots/interpolate}}
    }
\end{figure}
\setcounter{subfigure}{0}
\vspace{-1em}
Analytical and empirical priors.
\subref{F:dirichlet} shows the Dirichlet distribution order statistics versus the shape parameter; and
\subref{F:interpolate} shows the spectrum of $\Theta$ as a function of the 
rewiring parameter.
\end{frame}

\begin{frame}
\begin{figure}[h]
  \centering
  \subfigure[$m/\mu = 0.2$; $s = 4$.]{
    \makebox{\label{fig:perf-frob:a}
             \includegraphics[scale=0.24]{plots/estimation-frob-p020}}
  }
  \subfigure[$m/\mu = 1.0$; $s = 4$.]{
    \makebox{\label{fig:perf-frob:b}
             \includegraphics[scale=0.24]{plots/estimation-frob-p100}}
  }
  \subfigure[$m/\mu = 2.0$; $s = 4$.]{
    \makebox{\label{fig:perf-frob:c}
             \includegraphics[scale=0.24]{plots/estimation-frob-p200}}
  } \\
\vspace{-1.5em}
  \subfigure[$m/\mu = 2.0$; $s = 0$.]{
    \makebox{\label{fig:perf-frob:d}
             \includegraphics[scale=0.24]{plots/estimation-frob-s0-p200}}
  }
  \subfigure[$m/\mu = 2.0$; $s = 32$.]{
    \makebox{\label{fig:perf-frob:e}
             \includegraphics[scale=0.24]{plots/estimation-frob-s32-p200}}
  }
\setcounter{subfigure}{0}
\vspace{-1.5em}
\end{figure}
These plots show the relative Frobenius norm error, versus the (normalized) regularization parameter 
$\eta / \bar \tau$ for various values of the (normalized) number of edges, 
$m/\mu$, and the edge-swap parameter, $s$.
Smaller values along the X-axis correspond to stronger regularization.
\end{frame}

\begin{frame}
    \begin{center}
    \makebox{\label{F:optimal-frob}
             \includegraphics[scale=0.6]{plots/optimal}}
    \vspace{-2em}
  \end{center}
This figure plots the optimal regularization parameter 
$\eta^\ast / \bar \tau$ as a function of sample proportion for different
fractions of edge swaps.
\end{frame}

\begin{frame}
  \frametitle{References}
  \bibliographystyle{unsrt}
  \footnotesize{
    \bibliography{refs}
  }
\end{frame}

\end{document}
