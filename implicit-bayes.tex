\documentclass[12pt]{article}
\RequirePackage[OT1]{fontenc}
\RequirePackage{fullpage}
\RequirePackage{amsbsy,amsmath,amssymb,amsthm}
\RequirePackage{natbib}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\trans}{\mathrm{T}}
\DeclareMathOperator*{\Tr}{Tr}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\rank}{rank}
\newcommand{\Normal}[1][]{\mathcal{N}_{#1}}
\newcommand{\Wishart}[1][]{\mathcal{W}_{#1}}
\DeclareMathOperator*{\argmin}{argmin}

\newcommand{\prob}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}


\begin{document}

\title{
  Regularized Eigenvector Estimation
}
\author{
  Patrick O.\ Perry
  \and
  Michael W.\ Mahoney
}
\date{
  \today
}
\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

\section{Graph partitioning}
\label{S:introduction}

A weighted symmetric graph, $G$ is defined by a vertex set $V_G$, and a
weight function $w_G : V_G \times V_G \to \reals_+$, where $w_G$ is symmetric
in its arguments ($w_G(u,v) = w_G(v,u)$).  The cardinality of $V_G$,
denoted $|V_G|$, is finite.

A length-$l$ path in $G$, denoted $\gamma$, is defined by a finite sequence of
vertices $\gamma(1), \gamma(2), \ldots, \gamma(l)$, where $\gamma(i) \in V_G$; given
such a path, $\gamma(1)$ is called the start of $\gamma$ and 
$\gamma(l)$ is called terminus of $\gamma$.  With respect to
the graph, $G$, vertices $u$ and $v$ are said to be $l$-connected if there
exists a length-$l$ path $\gamma$ such that $\gamma(1) = u$,
$\gamma(l) = v$, and $w(\gamma(i), \gamma(i+1)) > 0$ for $i = 1,
\ldots, l-1$.  Vertices $u$ and $v$ are said to be connected if they
are $l$-connected for some positive $l$.  Connectivity defines an
equivalence relation on the vertices of a graph.  The equivalence
classes are called the connected components of $G$; they define a
natural partition of the vertices.

\subsection{Spectral clustering}

Given a weighted symmetric graph, $G$, one can construct a positive
semidefinite matrix, $L_G \in \reals^{V_G \times V_G}$, called the combinatorial Laplacian of $G$:
\[
  L_G(u,v)
  =
  \begin{cases}
    - w_G(u,v) & \text{when $u \neq v$,} \\
    d_G(v) - w_G(v,v) & \text{otherwise,}
  \end{cases}
\]
where $d_G(v) = \sum_{v'} w_G(v,v')$ is called the degree of $v$.
By construction, $L_G$ is positive semidefinite.  Also, the all ones
vector is an eigenvector of $L_G$ with eigenvalue zero, i.e. $L_G \, 1_{V_G}
= 0$, where $1_{V_G}(v) = 1$ for all $v$; we call $1_{V_G}$ the
trivial eigenvector of $L_G$.  If $C \subseteq V_G$ is a connected component of
$G$ and $1_C(v) = 1\{ v \in C \}$, then $L_G \, 1_C = 0$.  One can
show that the multiplicity of the zero eigenvalue is equal to the
number of connected components in $G$.

When $G$ has exactly two connected components, say $C$ and $\bar C$,
the combinatorial Laplacian $L_G$ has two orthogonal eigenvectors
with eigenvalue $0$: the trivial eigenvector and
$c \cdot 1_C - c^{-1} \cdot 1_{\bar C}$, where
$c = \sqrt{|\bar C| / |C|}$.  We can recover the vertex partiton
$V_G = C \sqcup \bar C$ by ``cutting'' along the first nontrivial
eigenvector of $L_G$: if $L_G x = 0$ and $1' x = 0$, then up to a sign
change in $x$, $C = \{ v : x(v) > 0 \}$ and
$\bar C = \{ v : x(v) < 0 \}$.

The idea of spectral clustering is to partition the vertices of a
weighted graph by using the nontrivial eigenvectors of a Laplacian
(either the combinatorial Laplacian, which we have defined above, or a
degree-normalized version).  For example, to partition the vertices
into two sets, one can take $x$, the first nontrivial eigenvector of
$L_G$, and for a given threshold $K$, define the two sets of the partition as
\begin{align*}
  C(x,K)      &= \{ i : x_i >= K \}, \quad \text{and} \\
  \bar C(x,K) &= \{ i : x_i < K \}.
\end{align*}
Note that this procedure defines a partion even when the vertices of
$G$ are all connected.  Cheeger's inequality gaurantees that for an
appropriate choice of $K$, the partition is well-behaved, according to a
measure called ``conductance.''


\subsection{Local spectral clustering}

Spectral clustering reduces a graph to a single vector---the smallest
nontrivial eigenvector of the graph's Laplacian---and then clusters
the nodes using the information in the vector.  It is possible to use
more information about the graph by using more than one eigenvector
from the Laplacian.  In particular the elements of the pseudoinverse
of the Laplacian give local (node-specific) information about random
walks on the graph.

Let $G$ be a graph with combinatorial Laplacian $L_G$, and let
$L_G^+$ denote its pseudoinverse. The pseudoinverse of the Laplacian is closely
related to random walks on the graph.  In particular, 
$L_{G}^{+}$ defines a proximity measure on the graph with the
following properties:
\begin{description}
  \item[Symmetry]
    $L_G^{+}(u,v) = L_G^{+}(v,u)$;
 \item[Diagonal maximality]
    $L_G^{+}(u,u) > L_G^{+} (u,v)$ whenever $u \neq v$;
  \item[Triangle inequality for proximities]
    If $u$ and $v$ are in the same connected component, then
    $L_G^{+} (u,v) + L_G^{+} (u,w) - L_G^{+} (v,w) \leq L_G^{+} (u,u)$, with strict inequality
    holding when $v = w$ and $u \neq v$;
  \item[Metric representability]
   $L_G^{+} (u,u) + L_G^{+} (v,v) - L_G^{+} (u,v) - L_G^{+} (v,u)$
    defines a metric on every connected component of $V_G$;  this is commonly called
    \emph{resistance distance} on $G$.
 \item[Transit property]
    if $G$ contains a path from $u$ to $v$, and each path from $u$ to
    $v$ includes $w$, distinct from $u$ and $v$, then $L_G^{+}(u,w) >
    L_G^{+}(u,v)$.
\end{description}
These properties follow from \citefullauthor{chebotarev1998proximity}'s
(\citeyear{chebotarev1998proximity}) topological representation of
$L_G^{+}$.  A full discusion of this representation is beyond the
scope of the present treatment, but for unweighted, connected graphs,
\citefullauthor{chebotarev1998proximity} show that if $|V_G| = n$,
then
\[
  L_G^{+}(u,v) = \frac{|\mathcal{F}_{n-2}^{uv}| - \frac{1}{n}
    \sum_{w} |\mathcal{F}_{n-2}^{uw}|}{|\mathcal{F}_{n-1}|},
\]
where $\mathcal{F}_k$ and $\mathcal{F}_k^{uv}$ are sets of subgraphs
of $G$; the set $\mathcal{F}_{k}$ comprises all rooted spanning forests
with $k$ edges, and $\mathcal{F}_{k}^{uv}$ comprises all rooted spanning
spanning forests with $k$ edges such that $u$ is a root, and $v$
belongs to the same tree as $u$.  \cite{chandra1989electrical} show that the
quantity
$L_G^{+} (u,u) + L_G^{+} (v,v) - L_G^{+} (u,v) - L_G^{+} (v,u)$ is
proportional to the length of time before a random walker started at
node $u$ reaches node $v$.  It is likely that $L_G^{+}(u,v)$ has a
probabalistic interpretaiton in terms of random walks on the graph,
but we do not know of any such interpretation.

Given a cutoff value, $K$, we can define a local partition around node
$u$ via $P_K(u) = \{ v : L_G^{+}(u,v) > K \}$.  Note that if $v$ is in
$P_K(u)$, then $u$ is in $P_K(v)$.  In light of the disconnection
condition, if the graph is disconnected, then there exists a $K$ such
that $u$ and $v$ are in the same connected component iff
$v \in P_K(u)$.  We call this partitioning ``local spectral
clustering.''

[Add Guattery example here, showing benefits of local spectral.]


\subsection{Diffusion clustering}

In practrice, the pseudoinverse of the Laplacian is seldom (if ever)
use to partition the nodes of a graph.  The more common alternatives
to spectral clustering involve random walks or diffusions on the
graph.  These approaches assign positive and negative to the nodes,
and then let the distribution of charge evolve according to dynamics
derived fromt the graph structure (see, e.g., \citet{andersen2006local}).
Three canonical evolution dynamics are the following
\begin{description}
  \item[heat kernel]
    charge evolves according to the dynamics of the heat equation
    $\frac{\partial H_t}{\partial t} = - L_G H_t$;
  \item[PageRank]
    charge at a node evolves by either teleporting to a random node or moving
    to a neighber of the current node;
  \item[lazy random walk]
    charge either stays at the current node or moves to a neighbor.
\end{description}
\citet{mahoney2010implementing} showed that the above dynamics arrise
as solution to semi-definite programs of the form
\[
\begin{aligned}
  & \underset{X}{\text{minimize}}
  & & \mathrm{tr}(L_G X) + F(X) \\
  & \text{subject to}
  & & X \succeq 0, \\
  & & & \mathrm{tr}(X) = 1, \\
  & & & \mathrm{tr}(X J) = 0,
\end{aligned}
\]
where $J = 1_{V_G} 1_{V_G}'$.  Notably, when $F = 0$, the solution
to the above SDP is $u u'$, where $u$ is the smallest nontrivial
eigenvector of $L_G$.  Thus, in some sense, the heat kernel, PageRank,
and lazy random walk dynamics can be seen as ``regularized'' versions
of spectral clustering, where the function $F$ is acting as a penalty
function, analogous to the penalty in ridge regression or LASSO.

In the sequal, we extend \citefullauthor{mahoney2010implementing}'s
result, showing that solutions to the regularized SDP can be
interpreted as regularized estimates of the pseudoinverse of the
Laplacian.  Specifically, we set up a sampling model, whereby the
Laplacian is interpreted as an observation from a random process.  We
posit the existence of a ``population Laplacian'' driving the random
process.  We then define an estimation problem: finding the inverse of
the population Laplacian.  It turns out that the solution to the
regularized SDP arrises as the maximium a posteriori probability (MAP)
estimate of the inverse of the population Laplacian.  The role of the
penalty funciton, $F$, is to encode prior assumptions about the
population Laplacian.

Our approach is reminiscent of the Bayesian interpretation of
regularized linear regression.  In linear regression, an $L^2$ penalty
corresponds to a Gaussian prior on the coefficent vector, and the
solution to the regularized regression problem is the maximium a
posterior (MAP) estimator.  In the same setting, $L^1$ regularization
corresponds to a Laplace prior on the coefficent vector.


\section{A statistical framework for graph estimation}


\subsection{Bayesian inference for the population Laplacian}

Let $G$ be a graph with vertex set $V = \{ 1, 2, \dotsc, n \}$, edge
set $E = V \times V$ equipped with the equivalence relation
$(u,v) = (v,u)$.  Let $\omega$ be an edge weight function, and let
$\mathcal{L}$ be the corresponding Laplacian.  Assume without loss of
generality that $\sum_{(u,v) \in E} \omega(u,v) = 1$.  We interpet
$\omega$ and $\mathcal{L}$ as unobserved ``population'' quantities,
referring to $\omega(u,v)$ as the population weight of edge $(u,v)$,
and referring to $\mathcal{L}$ as the population Laplacian.

Suppose we sample $m$ edges from the $E$, randomly chosen
according to the population weight function.  That is, we see edges
$(u_1, v_1), (u_2, v_2),  \dotsc, (u_m, v_m)$, where the edges are
all drawn independently and identically such that the probability of
seeing edge $(u,v)$ is
\[
  \prob_\omega\{ (u_1, v_1) = (u,v) \} = \omega(u,v).
\]
Notably, we will likely see duplicate edges, and not every edge with a
positive weight will get sampled.

We construct a weight function from the sampled edges, called the
sample weight function, $w$, defined such that
\[
  w(u,v) = \frac{1}{m} \sum_{i=1}^{m} 1\{ (u_i, v_i) = (u,v) \}.
\]
In turn, we construct a sample Laplacian, $L$, defined such that
\[
  L(u,v)
    =
    \begin{cases}
      \sum_{w \neq u} w(u,w) &\text{when $u = v$,} \\
      -w(u,v) &\text{otherwise.}
    \end{cases}
\]
Note that $\E_\omega[w(u,v)] = \omega(u,v)$ and $\E_\omega L =
\mathcal{L}$, where $\E_\omega$ denotes expecation with respect to
probability law $\prob_\omega$.

Our goal is to estimate the pseudoinverse of the population Laplacian,
$\mathcal{L}^{+}$, using only the sampled edges.  The obvious estimate
is $L^{+}$, the pseudoinverse of the sample Laplacian.  We can often
do better when we have prior information about $\mathcal{L}$.

The form of our prior information comes in the form of a prior distribution
for $\mathcal{L}$.  That is, we think of $\mathcal{L}$ as a random
object, drawn from some prior distribution.  If we think that
$\mathcal{L}$ comes from an ``expander-like'' graph, then our prior
distribution for $\mathcal{L}$ will put most of its weight on
expander-like graphs.  Correspondingly, if we think that $\mathcal{L}$
comes from a graph that can be embedded in a low-dimensional space,
then our prior distribution for $\mathcal{L}$ will put most if its
weight on such graphs.

Let $p(\mathcal{L})$ denote our prior for $\mathcal{L}$, assumed to be
an arbitrary density over the space of graph Laplacians.  Likewise, let
$p(L \mid \mathcal{L})$ denote the density for the
distribution of $L$, conditional on $\mathcal{L}$.  Using Bayes' rule,
the posterior destribution for $\mathcal{L}$ after having observed $L$
is proportional to $p(L \mid \mathcal{L}) \, p(\mathcal{L})$.
Conceptually, Bayes' rule gives us a straightforward to
incorporate prior information into our estimate of $\mathcal{L}^+$.
We can get an estimate of $\mathcal{L}^{+}$ be maximizing the
posterior distribution for $\mathcal{L}$.

\subsection{Approximate likelihood for the sample Laplacian}

The conditional density $p(L \mid \mathcal{L})$ is defined completely
by the edge sampling scheme we laid out above.   However, the exact
expression for $p(L \mid \mathcal{L})$ involves an intractible
combinatorial sum.  We appeal to a crude approximation for the conditional
density $p(L \mid \mathcal{L})$.  The approximaton works as follows:
\begin{enumerate}
\item For $i = 1, \dotsc, m$, define $x_i \in \reals^n$ such that
  \[
    x_i(u)
      =
      \begin{cases}
        +s_i &\text{when $u = u_i$,} \\
        -s_i &\text{when $u = v_i$,} \\
        0 &\text{otherwise,}
      \end{cases}
  \]
  where $s_i \in \{ -1, +1 \}$ is chosen arbitrarily.
\item Note that $L = \sum_{i=1}^m x_i x_i'$.
\item Take $s_i$ to be random, equal to $+1$ or $-1$ with probability
  $\tfrac{1}{2}$.  Approximate the distribution of $x_i$ by the
  distribution of a multivariate normal random variable, $\tilde x_i$,
  such that $x_i$ and $\tilde x_i$ have the same first and second
  moments.
\item Approximate the distribution of $L$ by the distribution of $\tilde L$, where
  \(
    \tilde L = \sum_{i=1}^m \tilde x_i \tilde x_i'.
  \)
\end{enumerate}

\noindent
The next two lemmas derive the distribution of $\tilde x_i$ and
$\tilde L$ in terms of $\mathcal{L}$, allowing us to get an
approximation for $p(L \mid \mathcal{L})$.

\begin{lemma}
  With $x_i$ and $\tilde x_i$ defined as above,
  \[
    \E_\omega[ x_i ] = \E_\omega[ \tilde x_i ] = 0,
  \]
  and
  \[
    \E_\omega[ x_i x_i' ] = \E_\omega [ \tilde x_i \tilde x_i' ] = \mathcal{L}.
  \]
\end{lemma}
\begin{proof}
  The random variable $\tilde x_i$ is defined to have the same first
  and second moments as $x_i$.
  The first momemnt vanishes since $s_i \overset{d}{=} -s_i$ implies
  that $x_i \overset{d}{=} -x_i$.  For the second moments, note that
  when $u \neq v$, 
  \[
    \E_\omega[x_i(u) \, x_i(v)]
      = -s_i^2 \, \prob_\omega\{ (u_i,v_i) = (u,v) \}  = -\omega(u,v)
      = \mathcal{L}(u,v).
  \]
  Likewise,
  \[
    \E_\omega[\{x_i(u)\}^2]
      = \sum_{v} \prob_\omega\{ (u_i,v_i) = (u,v) \}
      = \sum_{v} \omega(u,v)
      = \mathcal{L}(u,u).
  \]
\end{proof}

\begin{lemma}\label{L:approx-wishart}
  The random matrix $\tilde L$ is distrubted as a Wishart matrix with
  shape $\mathcal{L}$ and scale $m$.  This distribution, denoted
  $\mathcal{W}(\mathcal{L}, m)$ is supported on the set of
  positive-semidefinite matrices with the same nullspace as $\mathcal{L}$.  When
  $m \geq \rank(\mathcal{L})$, the distribution has a denisty on this space
  given by
  \begin{equation}\label{E:wishart-density}
    f_\mathcal{W}( \tilde L \mid \mathcal{L}, m)
      \propto
      \frac{|\tilde L|^{(m - \rank(\mathcal{L}) - 1)/2}
        \exp\{-\tfrac{1}{2} \Tr(\mathcal{L}^+ \tilde L) \}}
        {|\mathcal{L}|^{m/2}}
  \end{equation}
  where the constant of proportionality depends only on $m$ and $\rank(\mathcal{L})$,
  and where $|\cdot|$ denotes pseudodeterminant (product of nonzero
  eigenvalues).
\end{lemma}
\begin{proof}
  Since $\tilde L$ is a sum of $m$ outer products of multivariate
  $\mathrm{Normal}(0, \mathcal{L})$, it is Wishart distributed
  (by definition).
  Suppose $\rank(\mathcal{L}) = r$ and
  $U \in \reals^{n \times r}$ is a matrix whose columns are the
    eigenvectors of $\mathcal{L}$.  Note that
    $U' \tilde x_i \overset{d}{=} \mathrm{Normal}(0, U' \mathcal{L} U)$,
    and that $U' \mathcal{L} U$ has full rank.  Thus,
    \(
      U' \tilde L U
    \)
    has a density over the space of $r \times r$ positive-semidefinite
    matrices whenever $m \geq r$.  The density of $U' \tilde L U$ is
    exactly equal to $f_\mathcal{W}(\tilde L \mid \mathcal{L}, m)$,
    defined above.
\end{proof}


\subsection{Approximate posterior inference}

Lemma~\ref{L:approx-wishart} gives us an approximation for
$p(L \mid \mathcal{L})$.  Namely, we can approximate
$p(L \mid \mathcal{L})$ by
$\tilde p(L \mid \mathcal{L}) = f_\mathcal{W}(L \mid \mathcal{L}, m)$,
where $f_\mathcal{W}$ is as defined in Eq.~\eqref{E:wishart-density}.
With a specification for the prior density $p(\mathcal{L})$, we can
get an approximation to the posterior denisty:
\[
  p(\mathcal{L} \mid L)
  \approx
  \tilde p(\mathcal{L} \mid L)
    \propto \tilde p(L \mid \mathcal{L}) \, p(\mathcal{L}).
\]
We can then estimate $\mathcal{L}$ by maximizing
$\tilde p(\mathcal{L} \mid L)$.

Suppose the prior takes the form
\[
  p(\mathcal{L}) \propto \exp\{ - F(\mathcal{L}) \}
\]
for some function, $F$, supported on the space of
positive-semidefinite matrices

\clearpage

\section{Connection to covariance estimation}
\label{S:connection-to-covariance}

The relaxed eigenvector problems appears related to a standard covariance estimation problem.


Let $x_1, \ldots, x_n$ be i.i.d., drawn from $\mathrm{Normal}(0, \Sigma)$
for unknown $\Sigma$.  Matrix $\Sigma^{-1}$, is called the precision
matrix.  Set $\tau = \mathrm{Tr}(\Sigma^{-1})$ and
$\Phi = \tau^{-1} \Sigma^{-1}$.  Say we have prior information that the
eigenvectors of $\Phi$ are uniformly (Haar) distributed, and that the
eigenvalues are Dirichlet distributed with parameter vector
$(\alpha, \ldots, \alpha)$ for some $\alpha > 0$.  The prior density
of $\Phi$ over the space of positive-definite matrices given by
\[
    \pi(\Phi) \propto |\Phi|^{\alpha-1}
\]
(the proportionality constant depends only on $\alpha$ and $p$).
Assume a uniform prior for $\tau$.
Define
\[
    A = \sum_{i=1}^n x_i x_i^T,
\]
the unnormalized sample covariance matrix.  Given $\Phi$, matrix $A$
follows a Wishart distribution with shape $\Sigma$ and scale $n$.  Its
density is
\[
    f(A | \Phi, \tau)
        \propto
        \exp\{
            -\tfrac{\tau}{2} A \cdot \Phi
            + \tfrac{n}{2} \log | \Phi| + \tfrac{np}{2} \log \tau
        \}
\]
(here, the proportionality constant depends only on $A$ and $n$).  The
posterior distribution for $\Phi$ and $\tau$ is the proportional to the
product of the prior and the likelihood:
\[
    \pi(\Phi, \tau | A)
        \propto
        \exp\{
            -\tfrac{\tau}{2} A \cdot \Phi
            + \tfrac{n}{2} \log | \Phi| + \tfrac{np}{2} \log \tau
            +
            (\alpha - 1)
            \log |\Phi|
        \}
\]
Collecting logs, taking logs and multiplying by $-2$, the maximum a
posteriori (MAP) estimate of $\Phi$ and $\tau$ is the solution to the
optimization problem:
\begin{align*}
    \min \qquad &\tau A \cdot \Phi - (n + 2 (\alpha - 1)) \log |\Phi| + np \log \tau \\
    \text{s.t.} \qquad & I \cdot \Phi = 1 \\
    \phantom{\text{s.t.}} \qquad & \Phi \succeq 0 \\
    \phantom{\text{s.t.}} \qquad & \tau \geq 0
\end{align*}

The solution is
\[
    \Phi = - [\tfrac{1}{n + (2\alpha - 1)}(\lambda I - \tau A)]^{-1}
\]

\textit{\textbf{Issue (1). How is $\Phi$ a pagerank?.}
We can show under certain model/prior assumptions that the MAP estimator of the inverse covariance matrix is a ``pagerank,'' $P$, but the ``pagerank'' matrix has negative entries, so what exactly does that mean?  You brought up the idea of replacing $P$ by, say, $P + x x'$ for suitable chosen $x$ such that $P + x x'$ is positive, performing the pagerank surfer walk with $P + x x'$, and then projecting orthogonal to $x$.  I'm not sure if this makes sense, though, because different choices of $x$ will give different answers.}


\section{Relationship between covariance estimation and graph Laplacians}

\textit{\textbf{Issue (2). This section is pretty flimsy.} The connection between graph Laplacians and sample covariance matrices is tenuous.  There is some sparse, graph-like structure in the inverse covariance matrix which might be the connection, but that's more like an adjacency matrix, not a Laplacian.}

Suppose we have a graph, $\mathcal{G}$ with $n$ vertices.  When we
say that $\mathcal{G}$ is ``low-dimensional,'' we mean we can embed the
vertices in $\mathbb{R}^d$ with $d \ll n$ such that Euclidean distance
for the embedded points agrees with geodesic distance on the graph.  When
we say that $\mathcal{G}$ is an ``expander,'' we mean that no low-dimensional
Euclidean embedding preserves the distances.

These concepts have analogues for sequences of high-dimensional,
independent and identically distributed (i.i.d.) points.  Let $p$ be
a dimension, conceptually big.  Suppose $x_1, \ldots, x_n$ is a sequence
of independent Gaussian-distributed vectors in $\mathbb{R}^d$ with population
mean zero and covariance matrix $\Sigma \in \mathbb{R}^{p \times p}$ 
(which is positive semi-definite).  One way of generating such a sequence
is the following:
\begin{enumerate}
    \item draw $Z$ in $\mathbb{R}^{n \times p}$, a random matrix whose
        entries are independent Gaussians having mean $0$ and variance $1$;
    \item compute $\Sigma^{1/2}$ such that
        $[\Sigma^{1/2}] [\Sigma^{1/2}]^T = \Sigma$ and set
        $X = Z [\Sigma^{1/2}]^{T}$;
    \item take $x_1, \ldots, x_n$ to be the rows of $X$.
\end{enumerate}
If $\Sigma$ has low rank, $d$, then $x_1, \ldots, x_n$ can be projected
into a $d$-dimensional subspace such that distances between the points
are preserved.  The subspace is spanned by the $d$ eigenvectors of
$\Sigma$ corresponding to its nonzero eigenvalues.  If $\Sigma$ is
a scalar multiple of the identity matrix, then no such low-dimensional
projection exists; for $n \geq p$, the $x$'s span $\mathbb{R}^p$ with
probability $1$, and they are not concentrated in any low-dimensional
subspace [this is a little hand-wavy].

In general the ``low-dimensionality'' and ``expanderness'' of a
high-dimensional distribution is determined by the eigenvalues of $\Sigma$,
its covariance matrix.  The projection onto the eigenvector of $\Sigma$
corresponding to eigenvalue $\lambda$ captures fraction
$\lambda / \mathrm{Tr}(\Sigma)$ of the total variance.  For low-dimensional
distributions, most of the eigenvalues are zero.  For expanders, the
eigenvalues are all about equal.  For in-between, the eigenvalues decay
slowly to zero.


\section{Other diffusions}

Other choices of the penalty function, $F$ yield different diffusions.  One way to derive priors for these distrubutions is by working backwards.

\textit{\textbf{The resulting priors depend on the data.  Does this make sense?}}


One Bayesianization of the regularized eigenvector problem involves the Wishart
distribution on positive-definite matrices.  Recall that if $\Sigma$ is
positive-semidefinite and $x_1, \ldots, x_n$ are independent
$\Normal[d](0, \Sigma)$ random vectors, then the scaled sample covariance matrix
defined by $A = \sum_{i=1}^n x_i x_i^\trans$ is said to follow the $d$-dimensional
Wishart distribution with shape parameter $\Sigma$ and scale $n$.  We denote
this by $A \sim \Wishart[d](\Sigma, n)$.  If $\Sigma$ is positive-definite and $n \geq d$, then
$A$ has density over the set of positive-definite matrices given by
\[
    f_{\Wishart}(A \mid \Sigma, n)
    =
    \frac{
        |A|^{(n-d-1)/2} \exp\{-\tfrac{1}{2} \Sigma^{-1} \bullet A \}
    }{
        2^{dn/2} \pi^{d(d-1)/4}
        |\Sigma|^{n/2}
        \prod_{j=1}^{d} \Gamma\big(\tfrac{1}{2} (n + 1 - j)\big)
    },
\]
where $\Gamma(\cdot)$ is the Gamma function.  For $d > 1$ this generalizes the
chi-squared distribution.  If $S = \tfrac{1}{n} A$ is a sample covariance matrix
with $n$ degrees of freedom, then $S$ has density over the set of
positive-definite matrices equal to
\(
    n^d \cdot f_{\Wishart}(n S \mid \Sigma, n).
\)

Recall that the regularized eigenvector problem takes the form
\begin{equation}\label{E:evec-reg}
    \argmin_{\Phi}
        S \bullet \Phi
        +
        1/\eta \cdot F(\Phi),
\end{equation}
where $F$ has support on the set 
\(
    \{ \Phi : \Tr(\Phi) = 1, \Phi \succeq 0 \}.
\)
It is convenient to think of $S$ as a sample covariance matrix with $n$ degrees
of freedom, so that $A = n S$ is a Wishart matrix.
The form of~\eqref{E:evec-reg} suggests interpreting $\Phi^{-1}$ as the
expectation of $S$, so that $\Phi$ is is a precision matrix
(the inverse of a covariance matrix).

Solving \eqref{E:evec-reg} is equivalent to solving the following:
\begin{equation*}
    \argmin_{\Phi}
        \big[
            (n S) \bullet \Phi
            -
            (n-d-1) \log |n S|
            -
            n \log |\Phi|
        \big]
        +
        \big[
            n/\eta \cdot F(\Phi)
            +
            n \log |\Phi|
        \big].
\end{equation*}
Recognize this as the MAP estimator for $\Phi$ after observing $S$, a 
sample covariance matrix with $n$ degrees of freedom and expectation
$\Phi^{-1}$; the prior density for $\Phi$ is supported on the
set $\{ \Phi : \Tr(\Phi) = 1, \Phi \succeq 0\}$, and is given by
\begin{equation}\label{E:prior-phi}
    p_{F}(\Phi | n, \eta)
        \propto
        \frac{
            \exp\{-\tfrac{n}{2\eta} F(\Phi) \}
        }{
            |\Phi|^{n/2}
        }.
\end{equation}
We consider $n$ and $\eta$ to be a hyper-parameters.  As before, when
$\int p_{F}(\Phi | n, \eta) d\Phi$ diverges, the prior is improper.


\subsection{Priors corresponding to orthogonally invariant penalties}

When $F$ is orthogonally invariant, $p_F(\Phi | n, \eta)$ depends only on the
eigenvalues of $\Phi$.  Letting $\Phi = O \Lambda O^\trans$ be the
eigendecomposition of $\Phi$, our prior belief is that $O$ is Haar-distributed over
the group of $d\times d$ orthogonal matrices.  Set
$\Lambda = \diag(\lambda_1, \ldots, \lambda_d)$.  Under orthogonal invariance,
the prior $p_F(\Phi | n, \eta)$ is completely specified by defining
$p_F(\Lambda | n, \eta)$.  In what follows, we derive the priors corresponding
to the choices for $F$ given in Section~3 of ``Implementing regularization
implicitly\ldots''.

\subsubsection*{Generalized Entropy}

On the simplex, for $F_H$ being Generalized Entropy,
\begin{align*}
    F_H(\Lambda)
        &= \Tr(\Lambda \log \Lambda) - \Tr(\Lambda) \\
        &= \log \Big[ \prod_{j=1}^d \lambda_j^{\lambda_j} \Big] - 1.
\end{align*}
Thus, from \eqref{E:prior-phi} it follows that
\[
    p_{F_H}(\Lambda | n, \eta)
        \propto
            \prod_{j=1}^d
                \exp\Big\{
                    -\tfrac{n}{2}
                    \big(
                        \tfrac{\lambda_j}{\eta}
                        +
                        1
                    \big)
                    \log \lambda_j
                \Big\}.
\]
This distribution does not follow a recognizable parametric form.


\subsubsection*{Log-determinant}

With $F_D$ being Log-determinant,
\begin{align*}
    F_D(\Lambda)
        &= -\log |\Lambda| \\
        &= -\log \Big[ \prod_{j=1}^d \lambda_j \Big].
\end{align*}
Thus,
\[
    p_{F_D}(\Lambda | n, \eta)
        \propto
            \prod_{j=1}^d
                \lambda_j^{n \cdot (\eta^{-1} - 1)/2}.
\]
When $n \cdot (\eta^{-1} - 1) > -2$, this defines a proper density, specifically
that of the Dirichlet distribution with parameter vector $\alpha 1_d$, where
$\alpha = \tfrac{n}{2} \cdot (\eta^{-1} - 1) + 1$.


\subsubsection*{Standard $k$-norm}

For the Standard $k$-norm function,
\[
    F_k(\Lambda)
        = \frac{1}{k} \Tr(\Lambda^k)
        = \frac{1}{k} \sum_{j=1}^d \lambda_j^k
\]
Thus,
\[
    p_{F_k}(\Lambda | n, \eta)
        \propto
            \exp\Big\{
                -
                \frac{n}{2k \eta}
                \sum_{j=1}^{d} \lambda_j^k
                -
                \frac{n}{2}
                \sum_{j=1}^{d} \log \lambda_j
            \Big\}.
\]
As with Generalized Entropy, this does not correspond to a recognizable
distribution.



\section{Empirical evaluation}

\section{Concluding remarks}

\bibliographystyle{chicagoa}
\bibliography{refs.bib}

\end{document}
