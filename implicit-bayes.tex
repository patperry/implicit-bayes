\documentclass[12pt]{article}
\usepackage{nips11submit_e,times}
%\RequirePackage[OT1]{fontenc}
\RequirePackage{amsbsy,amsmath,amssymb,amsthm}
\RequirePackage{graphicx}
\RequirePackage{subfigure}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\trans}{\mathrm{T}}
\DeclareMathOperator*{\Tr}{Tr}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\rank}{rank}
\newcommand{\Normal}[1][]{\mathcal{N}_{#1}}
\newcommand{\Wishart}[1][]{\mathcal{W}_{#1}}
\DeclareMathOperator*{\argmin}{argmin}

\newcommand{\prob}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\DeclareMathOperator*{\RSS}{RSS}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}




\title{
  Regularized Laplacian Estimation
  and \\ Fast Eigenvector Approximation
}
\author{
  Patrick O.~Perry \\
  School of Engineering and Applied Sciences \\
  Harvard University \\
  Harvard, MA 02138 \\
  \texttt{patperry@seas.harvard.edu} 
  \And
  Michael W.~Mahoney \\
  Department of Mathematics \\
  Stanford University \\
  Stanford, CA 94305  \\
  \texttt{mmahoney@cs.stanford.edu}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\begin{abstract}
Recently, Mahoney and Orecchia demonstrated that popular diffusion-based 
procedures to compute a quick \emph{approximation} to the first nontrivial 
eigenvector of a data graph Laplacian \emph{exactly} solve certain 
regularized Semi-Definite Programs (SDPs). 
In this paper, we extend this result by providing a statistical 
interpretation of it.
Our interpretation will be analogous to the manner in which
$\ell_2$-regularized or $\ell_1$-regularized $\ell_2$-regression (often
called ridge regression and LASSO regression, respectively) can be
interperted in terms of a Gaussian prior or a Laplace prior, respectively,
on the coefficient vector of the regression problem.
Our framework will imply that the solutions to the Mahoney-Orecchia 
regularized SDP can be interpreted as regularized estimates of the 
pseudoinverse of the graph Laplacian.
Conversely, it will imply that the solution to this regularized estimation 
problem can be computed very quickly by running the fast diffusion-based 
PageRank procedure for computing an approximation to the first nontrivial 
eigenvector of the graph Laplacian.
Empirical results are also provided to illustrate the manner in which 
approximate eigenvector computation \emph{implicitly} performs statistical 
regularization, relative to running the corresponding exact algorithm.
\end{abstract}



\section{Introduction}
\label{sxn:intro}

Approximation algorithms and heuristic approximations are commonly used to
speed up the running time of algorithms in machine learning and data
analysis.
In some cases, the outputs of these approximate procedures are ``better''
than the output of the more expensive exact algorithms, in the sense that
they lead to more robust results or more useful results for the downstream
practitioner.
Recently, Mahoney and Orecchia formalized these ideas in the context of
computing the first nontrivial eigenvector of a graph
Laplacian~\cite{mahoney2010implementing}.
Recall that, given a graph $G$ on $n$ nodes or equivalently its $n \times n$
Laplacian matrix $L$, the top nontrivial eigenvector of the Laplacian
\emph{exactly} optimizes the Rayleigh quotient, subject to the usual
constraints.
This optimization problem can equivalently be expressed as a vector
optimization program with the objective function $f(x) = x^TLx$,
where $x$ is an $n$-dimensional vector, or as a Semi-Definite Program (SDP)
with objective function $F(X)=\mathrm{Tr}(L X)$, where $X$ is an $n \times n$
symmertic positive semi-definite matrix.
This first nontrivial vector is, of course, of widespread interest in
applications due to its usefulness for graph partitioning, image
segmentation, data clustering, semi-supervised learning, etc.~\cite{spielman96_spectral,guatterymiller98,ShiMalik00_NCut,BN03,Joa03,LLDM09_communities_IM}.

In this context, Mahoney and Orecchia asked the question: do popular
diffusion-based procedures---such as running the Heat Kernel or performing a
lazy random walk or computing the PageRank function---to compute a quick
\emph{approximation} to the first nontrivial eigenvector of $L$ solve some
other regularized version of the Rayleigh quotient objective function
\emph{exactly}.
Understanding this algorithmic-statistical tradeoff is clearly of interest
if one is interested in very large-scale applications, where performing
statistical analysis to derive an objective and then calling a black box
solver to optimize that objective exactly, might be too expensive.
Mahoney and Orecchia answered the above question in the affirmative, with
the interesting twist that the regularization is on the SDP formulation
rather than the usual vector optimization problem.
That is, these diffusion-based procedures exactly optimize the regularized
SDP $F(X)+\lambda G(X)$, for some regularization function $G(\cdot)$ to be
described below, subject to the usual constraints.

In this paper, we extend the Mahoney-Orecchia result by providing a
statistical interpretation of it.
Our interpretation will be analogous to the manner in which
$\ell_2$-regularized or $\ell_1$-regularized $\ell_2$-regression (often
called ridge regression and LASSO regression, respectively) can be
interperted in terms of a Gaussian prior or a Laplace prior, respectively,
on the coefficient vector of the regression problem.
In more detail, we will set up a sampling model, whereby the graph Laplacian
is interpreted as an observation from a random process; we will posit the
existence of a ``population Laplacian'' driving the random process; and we
will then define an estimation problem: find the inverse of the population
Laplacian.
The maximium a posteriori probability (MAP) estimate of the inverse of the
population Laplacian leads to a regularized SDP, where the objective
function $F(X)=\mathrm{Tr}(L X)$ and where the role of the penalty function
$G(\cdot)$ is to encode prior assumptions about the population Laplacian.
In addition, we show that if $G(\cdot)$ is the log-determinant function then
the MAP estimate leads to the Mahoney-Orecchia regularized SDP 
corresponding to running the PageRank heuristic.
Said another way, the solutions to the Mahoney-Orecchia regularized SDP can 
be interpreted as regularized estimates of the pseudoinverse of the graph 
Laplacian; and moreover, by Mahoney and Orecchia' main result, the solution 
to this regularized SDP can be computed very quickly---rather than solving 
the SDP with a black-box solver and rather computing explicitly the 
pseudoinverse of the Laplacian, one can simply run the fast diffusion-based 
PageRank heuristic for computing an approximation to the first nontrivial 
eigenvector of the Laplacian $L$.

The next section, Section~\ref{S:introduction}, describes the background on
the Laplacian and its pseudoinverse in light of local and global versions of
the basic spectral clustering applications that gave rise to this line of
work.
Section~\ref{snx:framework} then describes a statistical framework for graph
estimation; and Section~\ref{sxn:priors} describes prior assumptions that
can be made on the population Laplacian.
These two sections will shed light on the computational implications
associated with these prior assumptions; but more importantly they will shed
light on the implicit prior assumptions associated with making certain
decisions to speed up computations.
Section~\ref{sxn:empirical} will then provide a brief empirical evaluation,
demonstrating that diffusion-based approximation algorithms for the first 
nontrivial eigenvector of the graph Laplacian implicitly leads to 
statistical regularization, relative to the corresponding exact algorithm, 
in ways that should be familiar to the reader.
%%%NIPSSUB%%% and Section~\ref{sxn:conc} will provide a brief conclusion.




\section{Background on Laplacians and local and global graph partitioning}
\label{S:introduction}

Recall that a weighted symmetric graph, $G$, is defined by a vertex set 
$V = \{ 1, \dotsc, n \}$,
an edge set $E \subset V \times V$; 
and a weight function $w : E \to \reals_+$, where $w$ is assumed to 
be symmetric in its arguments (\emph{i.e.}, $w(u,v) = w(v,u)$).  
%%NIPSSUB%% 
%%%MWM%%% I DONT THINK WE USE THE FOLLOWING PATH LENGTH DISCUSSION, BUT IT MIGHT BE GOOD TO MENTION THE NOTION OF CONNECTIVITY, BUT THAT CAN PROBABLY BE ASSUMED OF THE READER.
%%%MWM%%% A length-$l$ path in $G$, denoted $\gamma$, is defined by a finite sequence of
%%%MWM%%% vertices $\gamma(1), \gamma(2), \ldots, \gamma(l)$, where $\gamma(i) \in V_G$; given
%%%MWM%%% such a path, $\gamma(1)$ is called the start of $\gamma$ and 
%%%MWM%%% $\gamma(l)$ is called terminus of $\gamma$.  With respect to
%%%MWM%%% the graph, $G$, vertices $u$ and $v$ are said to be $l$-connected if there
%%%MWM%%% exists a length-$l$ path $\gamma$ such that $\gamma(1) = u$,
%%%MWM%%% $\gamma(l) = v$, and $w(\gamma(i), \gamma(i+1)) > 0$ for $i = 1,
%%%MWM%%% \ldots, l-1$.  Vertices $u$ and $v$ are said to be connected if they
%%%MWM%%% are $l$-connected for some positive $l$.  Connectivity defines an
%%%MWM%%% equivalence relation on the vertices of a graph.  The equivalence
%%%MWM%%% classes are called the connected components of $G$; they define a
%%%MWM%%% natural partition of the vertices.
%%NIPSSUB%% 
%%NIPSSUB%% \subsection{Global spectral partitioning}
%%NIPSSUB%% 
%%NIPSSUB%% Given a weighted symmetric graph, $G$, 
In this case, one can construct a positive
semidefinite matrix, $L_0 \in \reals^{V \times V}$, called the 
combinatorial Laplacian of $G$:
\[
  L_0(u,v)
  =
  \begin{cases}
    - w(u,v) & \text{when $u \neq v$,} \\
    d(u) - w(u,u) & \text{otherwise,}
  \end{cases}
\]
where $d(u) = \sum_{v} w(u,v)$ is called the degree of $u$.
%%NIPSSUB%% XXX.  WE NEED TO BE CONSISTENT ABOUT COMBINATORIAL VERSUS NORMALIZED; IF WE GO WITH NORMALIZED LATER, PROBABLY BEST TO DO IT EVERYWHERE.
By construction, $L_0$ is positive semidefinite.  
Also, the all ones
vector is an eigenvector of $L_0$ with eigenvalue zero, i.e. $L 1 =
0$, where $1$ denotes the all ones vector; we call $1$ the
trivial eigenvector of $L_0$.  Letting $D$ be a diagonal matrix with
$D(u,u) = d(u)$, we can also define a normalized version of the
Laplacian: $L = D^{-1/2} L_0 D^{-1/2}$.  Unless explicitly stated
otherwise, when we refer to the Laplacian of a graph, this will mean
normalized Laplacian.

%%NIPSSUB%% If $C \subseteq V_G$ is a connected component of $G$ and 
%%NIPSSUB%% $1_C(v) = 1\{ v \in C \}$, then $L_G \, 1_C = 0$.  
%%NIPSSUB%% One can show that the multiplicity of the zero eigenvalue is equal to the
%%NIPSSUB%% number of connected components in $G$.
%%NIPSSUB%% When $G$ has exactly two connected components, say $C$ and $\bar C$,
%%NIPSSUB%% the combinatorial Laplacian $L_G$ has two orthogonal eigenvectors
%%NIPSSUB%% with eigenvalue $0$: the trivial eigenvector and
%%NIPSSUB%% $c \cdot 1_C - c^{-1} \cdot 1_{\bar C}$, where
%%NIPSSUB%% $c = \sqrt{|\bar C| / |C|}$.  
%%NIPSSUB%% Thus, we can recover the vertex partiton
%%NIPSSUB%% $V_G = C \sqcup \bar C$ by ``cutting'' along the first nontrivial
%%NIPSSUB%% eigenvector of $L_G$: if $L_G x = 0$ and $1' x = 0$, then up to a sign
%%NIPSSUB%% change in $x$, $C = \{ v : x(v) > 0 \}$ and
%%NIPSSUB%% $\bar C = \{ v : x(v) < 0 \}$.

The idea of spectral clustering is to approximate the 
best partition of the vertices of a connected graph by using the nontrivial 
eigenvectors of a Laplacian (either the combinatorial or the normalized).  
This idea has had a long history~\cite{Donath:1973,fiedler75B,spielman96_spectral,guatterymiller98,ShiMalik00_NCut}.
The simplest version of spectral clustering involves computing the first 
nontrivial eigenvector (or another vector with Rayleigh quotient close to 
that of the first nontrivial eigenvector) of $L$, and ``sweeping'' over 
that vector. 
%%NIPSSUB%% For example, 
%%NIPSSUB%% %%%MWM%%% to partition the vertices into two sets, 
%%NIPSSUB%% one can take that vector, call it $x$, and, for a given threshold $K$, 
%%NIPSSUB%% define the two sets of the partition as
%%NIPSSUB%% \begin{align*}
%%NIPSSUB%%   C(x,K)      &= \{ i : x_i >= K \}, \quad \text{and} \\
%%NIPSSUB%%   \bar C(x,K) &= \{ i : x_i < K \}.
%%NIPSSUB%% \end{align*}
%%NIPSSUB%% %%%MWM%%% Importantly, procedure defines a partion even when the vertices of $G$ are 
%%NIPSSUB%% %%%MWM%%% all connected.  
%%NIPSSUB%% XXX.  CITE FIRST FIG HERE, BUT PROBABLY HAVE A FIRST SUBFIG THAT GIVES THE USUAL 50-50 SPLIT AS WELL.
Typically, this vector is computed by calling a black-box solver, but it 
could also be approximated with an iteration-based method (such as the Power
Method or Lanczos Method) or a random walk-based method (such as running a 
diffusive procedure or PageRank-based procedure to the asymptotic state).
%%NIPSSUB%% Far from being a heuristic, 
This procedure provides a \emph{global 
partition} that (via Cheeger's inequality and for an appropriate choice of 
$K$) satisfies provable quality-of-guarantees with respect to the 
combinatorial problem of finding the best ``conductance'' partition in the 
entire graph~\cite{Mihail,spielman96_spectral,guatterymiller98}.

%%NIPSSUB%% \begin{figure}
%%NIPSSUB%%     \centering
%%NIPSSUB%%     \makebox{\includegraphics[scale=0.8]{plots/guattery-global}}
%%NIPSSUB%%     \caption{
%%NIPSSUB%%              Global partition from first nontrivial eigenvector of the Laplacian.
%%NIPSSUB%% XXX.  MAYBE TWO SUBFIGURES, ONE FOR A GRAPH WITH A GOOD BIPARTITION WHERE SPECTRAL WORKS AND THE GUATTERY ONE WHERE IT DOESNT.
%%NIPSSUB%% }
%%NIPSSUB%% \end{figure}
%%NIPSSUB%% 
%%NIPSSUB%% 
%%NIPSSUB%% \subsection{Local spectral partitioning}

%%NIPSSUB%% If spectral clustering reduces a graph to a single vector---the smallest
%%NIPSSUB%% nontrivial eigenvector of the graph's Laplacian---and then clusters the 
%%NIPSSUB%% nodes using the information in the vector, 
It is possible to obtain much 
more information about the graph by looking at more than one eigenvector 
from the Laplacian.  
In particular, the elements of the pseudoinverse of the combinatorial Laplacian, 
$L_0^{+}$, give local (\emph{i.e.}, node-specific) information about random
walks on the graph.
%%NIPSSUB%% The reason is that if $G$ is a graph with combinatorial Laplacian $L_G$, 
%%NIPSSUB%% then the pseudoinverse $L_G^+$  of the Laplacian is closely related to 
%%NIPSSUB%% random walks on the graph.  
%%NIPSSUB%% In particular, $L_{G}^{+}$ defines a proximity measure on the graph with the
%%NIPSSUB%% following properties:
%%NIPSSUB%% \begin{description}
%%NIPSSUB%%   \item[Symmetry]
%%NIPSSUB%%     $L_G^{+}(u,v) = L_G^{+}(v,u)$;
%%NIPSSUB%%  \item[Diagonal maximality]
%%NIPSSUB%%     $L_G^{+}(u,u) > L_G^{+} (u,v)$ whenever $u \neq v$;
%%NIPSSUB%%   \item[Triangle inequality for proximities]
%%NIPSSUB%%     If $u$ and $v$ are in the same connected component, then
%%NIPSSUB%%     $L_G^{+} (u,v) + L_G^{+} (u,w) - L_G^{+} (v,w) \leq L_G^{+} (u,u)$, with strict inequality
%%NIPSSUB%%     holding when $v = w$ and $u \neq v$;
%%NIPSSUB%%   \item[Metric representability]
%%NIPSSUB%%    $L_G^{+} (u,u) + L_G^{+} (v,v) - L_G^{+} (u,v) - L_G^{+} (v,u)$
%%NIPSSUB%%     defines a metric on every connected component of $V_G$;  this is commonly called
%%NIPSSUB%%     \emph{resistance distance} on $G$.
%%NIPSSUB%%  \item[Transit property]
%%NIPSSUB%%     if $G$ contains a path from $u$ to $v$, and each path from $u$ to
%%NIPSSUB%%     $v$ includes $w$, distinct from $u$ and $v$, then $L_G^{+}(u,w) >
%%NIPSSUB%%     L_G^{+}(u,v)$.
%%NIPSSUB%% \end{description}
See, \emph{e.g.}~\cite{chebotarev1998proximity} for details.  
%%%MWM%%% A full discusion of this
%%%MWM%%% These properties follow from Chebotarev and Shamis' topological
%%%MWM%%% representation of of the combinatorial Laplacian
%%%MWM%%% \cite{chebotarev1998proximity}.  A full discusion of this
%%%MWM%%% representation is beyond the scope of the present treatment, but for
%%%MWM%%% unweighted, connected graphs, the authors show that if $|V_G| = n$,
%%%MWM%%% then
%%%MWM%%% \[
%%%MWM%%%   L_G^{+}(u,v) = \frac{|\mathcal{F}_{n-2}^{uv}| - \frac{1}{n}
%%%MWM%%%     \sum_{w} |\mathcal{F}_{n-2}^{uw}|}{|\mathcal{F}_{n-1}|},
%%%MWM%%% \]
%%%MWM%%% where $\mathcal{F}_k$ and $\mathcal{F}_k^{uv}$ are sets of subgraphs
%%%MWM%%% of $G$; the set $\mathcal{F}_{k}$ comprises all rooted spanning forests
%%%MWM%%% with $k$ edges, and $\mathcal{F}_{k}^{uv}$ comprises all rooted spanning
%%%MWM%%% spanning forests with $k$ edges such that $u$ is a root, and $v$
%%%MWM%%% belongs to the same tree as $u$.  
%%NIPSSUB%% In addition, 
For example, 
it is known that the
%%%MWM%%% Chandra et al.\ show that the
quantity $L_0^{+} (u,u) + L_0^{+} (v,v) - L_0^{+} (u,v) - L_0^{+} (v,u)$ is
proportional to the length of time before a random walker started at
node $u$ reaches node $v$ whenever $u$ and $v$ are in the same
connected component~\cite{chandra1989electrical}.  The elements
of the pseodoinverse of the \emph{normalized} Laplacian give degree-scaled
measures of proximity between the nodes of a graph.
%%%MWM%%% It is likely that $L_G^{+}(u,v)$ has a probabalistic interpretation in terms 
%%%MWM%%% of random walks on the graph, but we are not aware of any such 
%%%MWM%%% interpretation.
%%NIPS%% 
From this perspective, given $L^{+}$ and a cutoff value, $K$, we can 
define a \emph{local partition} around node $u$ via 
$P_K(u) = \{ v : L^{+}(u,v) > K \}$.  
%%NIPSSUB%% (Note that if $v$ is in $P_K(u)$, then $u$ is in $P_K(v)$.  In addition, in 
%%NIPSSUB%% light of the disconnection condition, if the graph is disconnected, then 
%%NIPSSUB%% there exists a $K$ such that $u$ and $v$ are in the same connected component 
%%NIPSSUB%% iff $v \in P_K(u)$.)  
We call this partitioning \emph{local spectral clustering}.
Although the na\"{i}ve way of performing this partitioning, \emph{i.e.}, to 
compute $L^{+}$ explicitly, is prohibitive for anything but very small 
graphs, these ideas form the basis for very fast local spectral clustering 
methods that truncated diffusion-based procedures to compute localized 
vectors with which to partition; 
see~\cite{Spielman:2004,andersen06local,Chung07_heatkernelPNAS,MOV09_TRv2} 
for details.

%%NIPSSUB%% \begin{figure}
%%NIPSSUB%%     \centering
%%NIPSSUB%%     \makebox{\includegraphics[scale=0.6]{plots/guattery-local}}
%%NIPSSUB%%     \caption{Local partitions from the pseudoinverse of the Laplacian}
%%NIPSSUB%% \end{figure}
%%NIPSSUB%% \begin{figure}
%%NIPSSUB%%     \centering
%%NIPSSUB%%     \makebox{\includegraphics[scale=0.6]{plots/guattery-pagerank}}
%%NIPSSUB%%     \caption{Local partitions from PageRank}
%%NIPSSUB%% \end{figure}
%%NIPSSUB%% 
%%NIPSSUB%% [Add Guattery example here, showing benefits of local spectral.]
%%NIPSSUB%% 
%%NIPSSUB%% 
%%NIPSSUB%% \subsection{Approximating local and global spectral partitioning with 
%%NIPSSUB%% diffusion-based procedures}
%%NIPSSUB%% 
%%NIPSSUB%% Of course, the pseudoinverse of the Laplacian is seldom (if ever) used to 
%%NIPSSUB%% partition the nodes of a graph.
%%NIPSSUB%% Moreover, in certain cases, one might not call a black-box solver to comptue 
%%NIPSSUB%% the smallest nontrivial eigenvector of Laplacian exactly.
%%NIPSSUB%% In both of these cases, 
An extension to the basic spectral clustering 
involves running procedures based on random walks or diffusions 
%%%MWM%%% (started with a random vector or a deterministic vector with all 
%%%MWM%%% of its mass localized on one or a small number of seed nodes) 
on the graph.  
These approaches assign positive and negative ``charge'' to the nodes,
and then let the distribution of charge evolve according to dynamics
derived from the graph structure.
Three canonical evolution dynamics are the following.
\begin{description}
  \item[Heat Kernel.]
    Here, the charge evolves according to the dynamics of the heat equation
    $\frac{\partial H_t}{\partial t} = - L H_t$.
    Thus, the vector of charges evolves as
%%%NIPSSUB%%     \begin{equation}
%%%NIPSSUB%%     \label{eqn:heat-kernel}
%%%NIPSSUB%%     H_t = \exp ( -tL )  = \sum_{k=0}^{\infty} \frac{(-t)^k}{k!}L^k  ,
%%%NIPSSUB%%     \end{equation}
$     H_t = \exp ( -tL )  = \sum_{k=0}^{\infty} \frac{(-t)^k}{k!}L^k  $,
    where $t \ge 0$ is a time parameter, times an input seed distribution.
  \item[PageRank.]
    Here, the charge at a node evolves by either moving to a neighber of 
    the current node or teleporting to a random node.
    More formally, then the vector of charges evolves as 
    \begin{equation}
    \label{eqn:page-rank}
    R_{\gamma} = \gamma \left(I-\left(1-\gamma \right)M \right)^{-1}   ,
    \end{equation}
    where $M$ is the natural random walk transition matrix associated 
    with the graph and
    where $\gamma \in (0,1)$ is the so-called teleportation parameter,
    times an input seed distribution.
  \item[Lazy Random Walk.]
    Here, the charge either stays at the current node or moves to a neighbor.
    Thus, if $M$ is the natural random walk transition matrix associated 
    with the graph, then the vector of charges evolves as some power of 
%%%NIPSSUB%%     \begin{equation}
%%%NIPSSUB%%     \label{eqn:lazy-walk}
%%%NIPSSUB%%     W_{\alpha}= \alpha I + (1-\alpha)M ,
%%%NIPSSUB%%     \end{equation}
$     W_{\alpha}= \alpha I + (1-\alpha)M $,
    where $\alpha \in (0,1)$ represents the ``holding probability,'' times 
    an input seed distribution.
\end{description}
(These three variants may be found in recent work on local spectral graph 
partitioning~\cite{Spielman:2004,andersen06local,Chung07_heatkernelPNAS}.)
In each of these cases, there is a parameter ($t$, $\gamma$, and the number 
of steps of the lazy random walk) that controls the ``aggressiveness'' of 
the dynamics and thus controls how quickly the diffusive process 
equilibrates; and there is an input seed distribution.
If one is interested in global spectral partitioning, then this seed vector 
could be a vector with entries drawn from $\{-1,+1\}$ uniformly at random, 
while if one is interested in local spectral partitioning, then this vector 
could be the indicator vector of a small ``seed set'' of nodes.

%%NIPSSUB%% In either case, 
Mahoney and Orecchia showed that these three dynamics arise as solutions to 
%%NIPSSUB%% regularized 
SDPs of the~form
\begin{equation}
\begin{aligned}
  & \underset{X}{\text{minimize}}
  & & \mathrm{tr}(L X) + \tfrac{1}{\eta} G(X) \\
  & \text{subject to}
  & & X \succeq 0, \\
  & & & \mathrm{tr}(X) = 1, \\
  & & & X D^{1/2} 1 = 0,
\end{aligned}
\label{eqn:mo-reg-sdp}
\end{equation}
where $G$ is a penalty function (shown to be the 
generalized entropy, the log-determinant and a certain matrix-$p$-norm, 
respectively~\cite{mahoney2010implementing}) and where $\eta$ is a 
parameter related to the aggressiveness of the diffusive 
process~\cite{mahoney2010implementing}.  
Conversely, solutions to the SDP of~(\ref{eqn:mo-reg-sdp}) for appropriate 
values of $\eta$ can be computed exactly by running one of the above 
three diffusion-based procedures.
Notably, when $G = 0$, the solution to the SDP 
of~(\ref{eqn:mo-reg-sdp}) is $u u'$, where $u$ is the smallest nontrivial 
eigenvector of $L$.  
More generally and in this precise sense, the heat kernel, PageRank, and lazy 
random walk dynamics can be seen as ``regularized'' versions of spectral 
clustering and Laplacian eigenvector computation.
%%NIPSSUB%% XXX.  MWM CHANGED REGULARIZATION FXN FROM $F$ TO $G$ TO BE CONSISTENT WITH BEFORE, WEN NEED TO BE CONSISTENT EVERYWHERE.
Intuitively, the function $G(\cdot)$ is acting as a penalty function, in a 
manner analogous to the $\ell_2$ or $\ell_1$ penalty in ridge regression or 
LASSO, and by running one of these three dynamics one is \emph{implicitly} 
making assumptions about the form of $G(\cdot)$.
In this paper, we provide a statistical framework to make that intuition 
precise.

%%%MWM%%% In the sequal, we extend Mahoney and Orecchia's
%%%MWM%%% result, showing that solutions to the regularized SDP can be
%%%MWM%%% interpreted as regularized estimates of the pseudoinverse of the
%%%MWM%%% Laplacian.  Specifically, we set up a sampling model, whereby the
%%%MWM%%% Laplacian is interpreted as an observation from a random process.  We
%%%MWM%%% posit the existence of a ``population Laplacian'' driving the random
%%%MWM%%% process.  We then define an estimation problem: finding the inverse of
%%%MWM%%% the population Laplacian.  It turns out that the solution to the
%%%MWM%%% regularized SDP arrises as the maximium a posteriori probability (MAP)
%%%MWM%%% estimate of the inverse of the population Laplacian.  The role of the
%%%MWM%%% penalty function, $F$, is to encode prior assumptions about the
%%%MWM%%% population Laplacian.

%%%NIPSSUB%% (((
%%%NIPSSUB%% XXX.  INCORPORATE WITH SUBSXN BELOW OR REMOVE IF REDUNDANT.
%%%NIPSSUB%% Our approach is reminiscent of the Bayesian interpretation of
%%%NIPSSUB%% regularized linear regression.  In linear regression, an $L^2$ penalty
%%%NIPSSUB%% corresponds to a Gaussian prior on the coefficent vector, and the
%%%NIPSSUB%% solution to the regularized regression problem is the maximium a
%%%NIPSSUB%% posterior (MAP) estimator.  In the same setting, $L^1$ regularization
%%%NIPSSUB%% corresponds to a Laplace prior on the coefficent vector.
%%%NIPSSUB%% )))


\section{A statistical framework for regularized graph estimation}
\label{snx:framework}

Here we lay out a simple Bayesian framework for estimating a graph
Laplacian, which allows for regularization by incorporating prior
information.  We suppose that the observed graph Laplacian, $L$, is a
random object whose distrubution depends on a true ``population''
Laplacian, $\mathcal{L}$.  This induces a conditional density for $L$,
denoted $p(L \mid \mathcal{L})$.  Next, we assume prior information
about the population Laplacian in the form of a prior density,
$p(\mathcal{L})$.  Given the observed Laplacian, we estimate the
population Laplacian by maximizing its posterior density,
$p(\mathcal{L} \mid L)$.


\subsection{Analogy with regularized linear regression}
\label{S:regression}

It will be helpful to keep in mind the Bayesian interpretation of 
regularized linear regression.  
In that context, we observe $n$ predictor-response pairs in 
$\reals^p \times \reals$, denoted $(x_1, y_1), \dotsc, (x_n, y_n)$; 
the goal is to find a vector $\beta$ such that $\beta' x_i \approx y_i$.  
Typically, we choose $\beta$ by minimizing the residual sum of squares,
\emph{i.e.}, $F(\beta)=\RSS(\beta) = \sum_i \| y_i - \beta' x_i \|_2^2$, or 
a penalized version of it.  
For ridge regression, we minimize $F(\beta) + \lambda \|\beta\|_2^2$; while 
for LASSO regression, we minimize $F(\beta) + \lambda \|\beta\|_1$.  
The additional terms in the optimization criteria ($\lambda \|\beta\|_2^2$ 
and $\lambda \|\beta\|_1$) are called penalty functions.

Adding a penalty function to the optimization criterion can be
interpreted as incorporating prior information about $\beta$.  We
model $y_1, \dotsc, y_n$ as independent random observations with
distributions dependent on $\beta$.  Specifically, we suppose $y_i$ is
a Gaussian random variable with mean $\beta' x_i$ and known variance
$\sigma^2$.  This induces a conditional density for the vector
$y = (y_1, \dotsc, y_n)$:
\begin{equation}\label{E:regression-density}
  p(y \mid \beta)
    \propto
     \exp\{ -\tfrac{1}{2 \sigma^2} F(\beta) \},
\end{equation}
where the constant of proportionality depends only on $y$ and $\sigma$.
Next, we assume that $\beta$ itself is random, drawn from a
distribution with density $p(\beta)$.  This distribution is called a
prior, since it encodes prior knowledge about $\beta$.  Without loss
of generality, the prior density can be assumed to take the form
\begin{equation}\label{E:regression-prior}
  p(\beta) \propto \exp\{ -U(\beta) \}.
\end{equation}
Since the two random variables are dependent, upon observing $y$, we
have information about $\beta$.  This information is encoded in the
posterior density, $p(\beta \mid y)$, computed via Bayes' rule as
\begin{equation}\label{E:regression-posterior}
  p(\beta \mid y)
    \propto p(y \mid \beta) \, p(\beta)
    \propto \exp\{ -\tfrac{1}{2 \sigma^2} F(\beta) - U(\beta) \}.
\end{equation}
The maximum a posterior probablility (MAP) estimate of $\beta$ is the value 
that maximizes $p(\beta \mid y)$.  
Equivalently, $\beta$ minimizes $\log p(\beta \mid y)$.  
In this framework, we can recover the solution to Ridge regression or LASSO 
regression by setting
$U(\beta) = \tfrac{\lambda}{2 \sigma^2} \| \beta \|_2^2$ or
$U(\beta) = \tfrac{\lambda}{2 \sigma^2} \| \beta \|_1$, respectively.  
Thus, Ridge regression can be interpeted as imposing a Gaussian prior on 
$\beta$, and LASSO regression can be interpreted as imposing a 
double-exponential prior on $\beta$.


\subsection{Bayesian inference for the population Laplacian}
\label{S:bayesian-laplacian}

Suppose $L$ is the normalized Laplacian of a connected graph with $n$
nodes.  Take $L$ to be a random object with expectation~$\mathcal{L}$,
where $\mathcal{L}$ is another normalized graph Laplacian.  We refer to $\mathcal{L}$ as the
``population'' graph Laplacian, and we refer to $L$ as the ``sample''
graph Laplacian.  
%%%NIPSSUB%% XXX.  MWM NOTE: IN THE FINAL LONG VERSION, WE SHOULD MAKE SURE TO PRESENT EVERYTHING SO IT IS CLEAR WHY WE MAKE THESE VARIOUS ASSUMPTIONS AT VARIOUS PLACES.
In general, $\mathcal{L}$ can be distinct from $L$,
but we require that the nodes in the population and sample graphs have
the same degrees, given by the vector $d = \big(d(1), \dotsc,
d(n)\big)$. Let $D = \diag\big(d(1), \dotsc, d(n)\big)$ and let
$\mathcal{X} = \{ X : X \succeq 0, \, X D^{1/2} 1 = 0, \, \rank(X) = n - 1 \}$.  The
population and sample Laplacians are both members of $\mathcal{X}$.


To apply the Bayesian formalism, we need to fully specify the conditional 
density of $L$ given $\mathcal{L}$.  
In Sec.~\ref{S:regression}, in the context of linear regression, we assumed 
that the observations followed a Gaussian distrubition.  
A graph Laplacian is not a single observation---it is a positive
semidefinite matrix with a very specific structure.  To model $L$, we
choose a distribution for positive semi-definite matrices analogous to
the Gaussian distribution: a scaled Wishart matrix with expectation
$\mathcal{L}$.  
Although this distribution captures the trait that $L$ is positive 
semi-definite, it does not accurately model every feature of $L$, 
\emph{e.g.}, a scaled Wishart matrix does not necessarily have ones along 
its diagonal.  
However, the mode of the density is at $\mathcal{L}$, a Laplacian, and for 
large values of the scale parameter, most of the mass will be on matrices 
close to $\mathcal{L}$.
See the Supplementary Material for a more detailed heuristic justification
for the use of a Wishart distribution.

Let $m \geq n - 1$ be a scale parameter
and suppose $L$ is distributed as a
$\tfrac{1}{m} \mathrm{Wishart}(\mathcal{L}, m)$ random variable.
Thus, $\E[L \mid \mathcal{L}] = \mathcal{L}$ and $L$ has conditional density
\begin{equation}\label{E:density}
  p(L \mid \mathcal{L})
    \propto
      \frac{\exp\{ -\frac{m}{2} \Tr(L \mathcal{L}^+)\}}
           {|\mathcal{L}|^{m/2}},
\end{equation}
where $|\cdot|$ denotes pseudodeterminant (product of nonzero
eigenvalues).  The constant of proportionality depends only on $L$,
$d$, $m$, and $n$, and the density is supported on $\mathcal{X}$.
Eq.~\eqref{E:density} is analogous to Eq.~\eqref{E:regression-density}
in the linear regression context, with $1/m$ playing the role of
$\sigma^2$.

Now suppose we have know that $\mathcal{L}$ is a random object drawn from a 
prior density $p(\mathcal{L})$.
Without loss of generality,
\begin{equation}
\label{E:prior}
  p(\mathcal{L}) \propto \exp\{ -U(\mathcal{L}) \},
\end{equation}
for some function $U$, supported on a subset
$\mathcal{S} \subseteq \mathcal{X}$.
%%%POP%%% (The space $\mathcal{X}$ is
%%%POP%%% closed under inversion, so if $\mathcal{L}^{+} \in \mathcal{S}$, then
%%%POP%%% $\mathcal{L} \in \mathcal{X}$.)
Eq.~\eqref{E:prior} is analogous to Eq.~\eqref{E:regression-prior} from the 
linear regression example.

Upon observing $L$, the posterior distribution for $\mathcal{L}$ is
\begin{equation}\label{E:posterior}
  p(\mathcal{L} \mid L)
    \propto p(L \mid \mathcal{L}) \, p (\mathcal{L})
    \propto \exp\{ -\tfrac{m}{2} \Tr(L \mathcal{L}^+)
                   + \tfrac{m}{2} \log |\mathcal{L}^+|
                   - U(\mathcal{L}) \},
\end{equation}
with support determined by $\mathcal{S}$.  
Eq.~\eqref{E:posterior} is analogous to Eq.~\eqref{E:regression-posterior} 
from the linear regression example.
Denote by $\mathcal{\hat L}$ the MAP estimate of $\mathcal{L}$.
It follows that $\mathcal{\hat L}^+$ is the solution to the program
\[
\begin{aligned}
  & \underset{X}{\text{minimize}}
  & & \Tr(L X) + \tfrac{2}{m} U(X^+) - \log |X| \\
  & \text{subject to}
  & & X \in \mathcal{S}.
\end{aligned}
\]
Note the similarity with Mahoney and Orecchia's relaxed SDP 
of~(\ref{eqn:mo-reg-sdp}).
In particular, if
\(
  \mathcal{S} = \{ X : \Tr(X) = 1 \} \cap \mathcal{X},
\)
then the two programs are identical except for the factor of
$\log |X|$ in the optimization criterion.

%More generally, if 
%$\mathcal{S} = \{ X : \Tr(X) = \tau^{-1} \} \cap \mathcal{X}$, then
%the MAP estimator of
%$[\Tr(\mathcal{L}^{+})]^{-1} \mathcal{L}^{+} = \tau \mathcal{L}^+$ is
%the solution to the program
%\[
%\begin{aligned}
%  & \underset{X}{\text{minimize}}
%  & & \Tr(L X) + \tfrac{\tau}{m} F(\tau^{-1} X) - \tau \log |X| \\
%  & \text{subject to}
%  & & X \succeq 0 \\
%  & & & X D^{1/2} 1 = 0 \\
%  & & & \Tr(X) = 1
%\end{aligned}
%\]


\section{A prior related to the PageRank matrix}
\label{sxn:priors}

%%NIPSSUB%% To successfully leverage the results in the preceding section, we need
%%NIPSSUB%% to choose densities $p(\mathcal{L})$ that encode our prior beliefs in
%%NIPSSUB%% particular application regimes.

%%NIPSSUB%% In this section, we present a
%%NIPSSUB%% parametric form for the prior that allows us to interpolate between
%%NIPSSUB%%``expander-like'' graphs and ``low-dimensional'' graphs.  For a
%%NIPSSUB%% network to be expander-like, we expect the gaps between consecutive
%%NIPSSUB%% eigenvalues of the Laplacian to be small.  For a network to be
%%NIPSSUB%% low-dimensional, we expect the smallest non-trivial eigenvalues to be
%%NIPSSUB%% well-separated from the rest of the spectrum (simulations in
%%NIPSSUB%% Sec.~\ref{sxn:empirical} formalize this intuition).
To leverage the estimation framework in Sec.~\ref{snx:framework},
we need a prior distribution for the population Laplacian.  In
Secs.~\ref{S:prior-support}--\ref{S:prior-density} we define such a
prior based on neutrality and invariance conditions.
Then, in Sec.~\ref{S:posterior-density}, we analyize the MAP estimate
of $\mathcal{L}$ for this prior, which turns out to be related to the
PageRank matrix via the Mahoney-Orecchia SDP.

%%%NIPSSUB%% XXX.  I THINK WE SHOULD DO THINGS IN THE REVERSE ORDER.  THAI IS, I THINK WE SHOULD SAY THAT WE WANT PRIORS THAT CORRESPOND TO THE MO-REGULARIZED-SDP; THEN JUST GIVE THEN AND SHOW WHAT SDPS WE GET; THEN SAY FOR HEAT KERNEL AND LAZY WALK WE ARE ALMOST THERE BUT FOR PAGERANK WE GET EXACTLY WHAT WE HAD BEFORE; AND ONLY THEN DRAW CONNECTIONS TO LOW-DIM AND EXPANDERS, INCLUDING WHY IN TERMS OF SOCIAL GRAPHS THAT IS OF INTEREST AND A DISCUSSION OF THE ANALYTICAL AND EMPIRICAL PRIORS.  AND THEN MOVE ONTO THE EMPIRICAL RESULTS SECTION.  I THINK IT MAKES MORE SENSE IN TERMSOF LOCICAL FLOW AND RIGOR.

\subsection{Prior support set}
\label{S:prior-support}

The prior is supported on a subset of positive-semidefinite
matrices.  The support set is $\mathcal{X}$, the same support set as
for the conditional density defined in
Sec.~\ref{S:bayesian-laplacian}, so that every matrix in the support
set is positive semi-definite, has rank $n - 1$, and satisfies
$X D^{1/2} 1 = 0$.  Note that because the prior depends on the data
(via the orthogonality constraint induced by $D$), this is not a prior
in the fully Bayesian sense.  Instead the prior can be considered as
part of an empirical or pseudo-Bayes estimation procedure.

\subsection{Prior density}
\label{S:prior-density}

The prior we specify depends only on the eigenvalues of the normalized
Laplacian.  Equivalently, it depends only on the eigenvalues of the pseudoinverse of
the Laplacian.  If $\mathcal{L}^+ = \tau \, O \Lambda O'$ is the spectral
decomposition of the pseudoinverse of normalized Laplacian
$\mathcal{L}$, where $\tau \geq 0$,
$O \in \reals^{n \times n -1}$, 
$\Lambda = \diag\big(\lambda(1), \dotsc, \lambda({n-1})\big)$, and
$\sum_v \lambda(v) = 1$,  then the prior
weight on $\mathcal{L}$ only depends on $\tau$ and $\Lambda$.  Note that the
values $\lambda(1), \dotsc, \lambda(n-1)$ are unordered.

The implication is that the prior is ``nearly'' rotationally
invariant, i.e. $p(P' \mathcal{L} P) = p(\mathcal{L})$ for any rank-$(n-1)$
projection matrix $P$ satisfying $P D^{1/2} 1 = 0$.

The vector $\lambda = \big(\lambda(1), \dotsc, \lambda(n-1)\big)$ lies
in the unit simplex.  If we require that the
distribution for $\lambda$ be exchangeable (invariant under
permutations) and neutral ($\lambda(v)$ independent of the vector
$\big(\lambda(u) / (1 - \lambda(v)) : u \neq v\big)$ for all $v$),
then the only non-degenerate possibility is that $\lambda$ is
Dirichlet-distributed with parameter vector $(\alpha, \ldots, \alpha)$
\cite{fabius1973two}.  The parameter $\alpha$, which we refer to as
the ``shape'' parameter, must satisfy $\alpha > 0$ for the density to
be defined.  In this case,
\begin{equation}\label{E:dirichlet-prior}
  p(\mathcal{L})
   \propto p(\tau) \prod_{v=1}^{n-1} \lambda(v)^{\alpha - 1},
\end{equation}
where $p(\tau)$ is a prior for $\tau$.



\subsection{Posterior estimation and connection to PageRank}
\label{S:posterior-density}

Next, we analyze the MAP estimate associated with the prior and
explain its connection to PageRank.

\begin{proposition}\label{P:map-sdp}
  Suppose the conditional likilihood for $L$ given $\mathcal{L}$ is as
  defined in \eqref{E:density} and the prior density for $\mathcal{L}$
  is as defined in \eqref{E:dirichlet-prior}.  If
  $\mathcal{\hat L}$ is the MAP estimate of $\mathcal{L}$, then
  there exists an $\eta$ such that
  $[\Tr(\mathcal{\hat L}^+)]^{-1} \mathcal{\hat L}^+$ solves the
  Mahoney-Orecchia SDP \eqref{eqn:mo-reg-sdp} with $G(X) = -\log |X|$.
\end{proposition}

\begin{proof}
For $\mathcal{L}$ in the support set of the posterior,
define $\tau = \Tr(\mathcal{L}^+)$ and $\Theta  = \tau^{-1}
\mathcal{L}^+$, so that $\Tr(\Theta) = 1$.  Further, $\rank(\Theta) = n - 1$.
Express the prior in the form of Eq.~\eqref{E:prior} with function
$U$ given by
\[
  U(\mathcal{L})
    = -\log \{ p(\tau) \, |\Theta|^{\alpha - 1} \}
    = -(\alpha - 1) \log |\Theta| - \log p(\tau),
\]
where as before, $|\cdot|$ denotes pseudodeterminant.  Using
\eqref{E:posterior} and the relation
$|\mathcal{L}^+| = \tau^{n-1} |\Theta|$, the posterior
density for $\mathcal{L}$ given $L$ is
\[
  p(\mathcal{L} \mid L)
    \propto
      \exp\Big\{
        -\tfrac{m \tau}{2} \Tr(L \Theta)
        +\tfrac{m + 2(\alpha - 1)}{2} \log | \Theta |
        + g(\tau)\Big\},
\]
where
\(
  g(\tau)
    = \tfrac{m (n-1)}{2} \log \tau
        + \log p(\tau).
\)
Suppose $\mathcal{\hat L}$ maximizes the
posterior likelihood.  Define $\hat \tau = \Tr(\mathcal{\hat L}^+)$
and $\hat \Theta = [\hat \tau]^{-1} \mathcal{\hat L}^{+}$.
In this case, $\hat \Theta$ must minimize the quantity
\(
  \Tr(L \hat \Theta) - \tfrac{1}{\eta} \log |\hat \Theta|,
\)
where
\begin{equation}\label{E:eta}
  \eta
    = \frac{m \hat \tau}{m + 2(\alpha - 1)}.
\end{equation}
Thus $\hat \Theta$ solves \eqref{eqn:mo-reg-sdp} with $G(X) = - \log |X|$.
\end{proof}

Mahoney and Orecchia showed that the solution to
\eqref{eqn:mo-reg-sdp} with $G(X) = - \log |X|$ is closely related to
the PageRank matrix, $R_\gamma$ defined in \eqref{eqn:page-rank}.
Combining Prop.~\ref{P:map-sdp} with their result, we get that the MAP
estimate of $\mathcal{L}$ satisfies
\( \mathcal{\hat L}^+ \propto D^{-1/2} R_\gamma D^{1/2}; \)
conversely,
\[
  R_\gamma
    \propto
      D^{1/2} \mathcal{\hat L}^+ D^{-1/2}.
\]
Thus, the PageRank matrix
can be viewed as a degree-scaled regularized estimate of the
pseudoinverse of the Laplacian.  In this sense, when we use PageRank
for local graph partitioning, we are using a \emph{regularized}
estimate of the pseudoinverse of the Laplacian.  



\section{Empirical evaluation}
\label{sxn:empirical}

Here, we evaluate the performance of the regularized Laplacian
estimator compared to the unregularized estimator.  To do so, we need
a ground truth population Laplacian $\mathcal{L}$ and a noisily
obserserved sample Laplacian $L$. In Sec.~\ref{S:prior-evaluation}, we
construct a distribution for $\mathcal{L}$ able to represent both
low-dimensional graphs and expanders.  It turns out that the
prior~\eqref{E:dirichlet-prior} captures some of the qualitative
features of these graphs.  In Sec.~\ref{S:estimation}, we describe a
sampling procedure for $L$ which, superficially, has no relation to
the scaled Wishart conditional density~\eqref{E:density}.  Despite
this model misspecification, the regularized estimator $\hat L_{\eta}$
outperforms $L$ for many choices of the regularization parameter
$\eta$.  Finally, in Sec.~\ref{S:optimal}, we investigate how the
optimal choice of $\eta$ depends on properties of the population and
sample Laplacians.  We recover a result in direct agreement with
~\eqref{E:eta}.

We compute the regularized estimate $\mathcal{\hat L}_\eta$ up to a
constant of proportionality by solving the Mahoney-Orecchia
SDP~\eqref{eqn:mo-reg-sdp} with $G(X) = -\log|X|$.  We define the
unregularized estimate $\hat L$ to be equal to the observed Laplacian,
$L$.  Given a population Laplacian $\mathcal{L}$, we define
$\tau = \tau(\mathcal{L}) = \Tr(\mathcal{L}^+)$ and
$\Theta = \Theta(\mathcal{L}) = \tau^{-1} \mathcal{L}^+$. We define
$\hat \tau_\eta$, $\hat \tau$, $\hat \Theta_\eta$, and $\hat \Theta$
similarly to to the population quantities.  Our performance
criterion is relative Frobenius error
$\|\Theta - \hat \Theta_\eta\|_\mathrm{F} / \|\Theta - \hat \Theta
\|_\mathrm{F}$, where $\|\cdot\|_\mathrm{F}$ denotes Frobenius norm
($\|A\|_\mathrm{F} = [\Tr(A' A)]^{1/2}$).


\subsection{Ground truth generation and prior evaluation}\label{S:prior-evaluation}


To generate a ground truth population Laplacian, $\mathcal{L}$---equivalently, a
population graph---we start two-dimensional lattice with width $w$ and
height $h$, and $n = w h$ nodes.  Points in the lattice are connected
to their four nearest neighbors, making adjustments as necessary the
boundary.  We then perform $s$ edge-swaps: for each swap, we choose
two edges uniformly at random and then we swap the endpoints.  For
example, if we sample edges $i_1 \sim j_1$ and $i_2 \sim j_2$, then we
replace these edges with $i_1 \sim j_2$ and $i_2 \sim j_1$.  When
$s = 0$, the graph is a low-deminsional.  As $s$ increases to
infinity, the graph becomes more and more like a uniformly chosen
$4$-regular graph (an expander).  Indeed each edge swap is a step of
the Metropolis algorithm towards a uniformly chosen random graph with
a fixed degree sequence.  By varying the number of swaps,
we interpolate between a low-dimensional graph and an expander.




\begin{figure}
    \centering
    \subfigure[Dirichlet distriubtion order statistics]{
      \makebox{\label{F:dirichlet}\includegraphics[scale=0.6]{plots/dirichlet}}
    }
    \subfigure[Interpolating from a low-dimensional graph to an expander]{
      \makebox{\label{F:interpolate}\includegraphics[scale=0.6]{plots/interpolate}}
    }
    %\caption{Analytical and emprical priors}
\end{figure}

We evaluate the prior~\eqref{E:dirichlet-prior} by visually comparing
its expected order statistics with the expected eigenvalues of
$\Theta$.  In Fig~\ref{F:dirichlet}, we display the behavior of the order
statistics of $\lambda$ as $\alpha$ varies.  We fix the value $n = 42$
and then let $\alpha$ vary between $0.2$ and and $2$.  For each value
of the shape $\alpha$, we generate a random $(n-1)$-dimensional Dirichlet
vector, $\lambda$, with parameter vector $(\alpha, \dotsc, \alpha)$.
We compute the $n-1$ order statistics of $\lambda$ by sorting its
components.  We repeat the procedure for $500$ replicates and then
average the values.  For small values of $\alpha$, we observe that the
first few order-statistics are well-separated from the rest.  As
$\alpha$ increases, the order statistics become concentrated
around $1/(n-1)$.

Fig.~\ref{F:interpolate} shows a corresponding plot for the ordered
eigenvalues of $\Theta$.
For each value of $s$, we generate the normalized
Laplacian, $\mathcal{L}$, corresponding to the random $s$-swapped
grid.  We compute the $n-1$ nonzero eigenvalues of $\Theta$.  Finally,
we perform $1000$ replicates of the procedure and average the
resulting eigenvalues. We can see that when $s = 0$, the top two eigenvalues
(corresponding to the width-wise and height-wise coordinates on the
grid) are well separated from the bulk.  As we increase $s$, the top
eigenvalues merge into the bulk; eventually, as $s$ goes to infinity,
the distribution will be very close that that of a uniformly chosen
$4$-regular graph.  We observe that the behavior is qualitatively
similar to the behavior of the Dirichlet prior order statistics as the
shape parameter $\alpha$ increases.


\subsection{Sampling procedure and estimation performance}\label{S:estimation}

Next, we evaluate the estimation performance of a regularized
estimator of the graph Laplacian as compared to an unregularized
estimate.  We construct the population graph $\mathcal{G}$ and its
Laplacian $\mathcal{L}$ as in Sec.~\ref{S:prior-evaluation}.  The
sampling procedure used to generate the observed graph $G$ and its
Laplacian $L$ is parameterized by $m$, the sample size.  This
parameter is analogous to the Wishart scale parameter
in~\eqref{E:density}, but importantly, we are sampling from a
different distribution.

We randomly choose $m$ edges with replacement from $\mathcal{G}$.  We
define sample graph $G$ and corresponding Laplacian by setting the
weight of $i \sim j$ to the number of times we sampled that edge.  The
sample graph $G$ overcounts some edges in $\mathcal{G}$ and misses
others.

Fig~\ref{F:estimation-frob} shows how the Frobenius error behaves when
$s = 4$ and $m$ is chosen such that $G$ and $\mathcal{G}$ have the
same number of edges (counting multiplicity).  The plot shows the mean
error and one standard deviation around it are plotted as a function
of $\eta/\bar \tau$, as computed from 100 replicates.  Here,
$\bar \tau$ is the mean value of $\tau$ over all replicates.We can see
that regularization improves the performance of the estimator for a
large range of $\eta$ values.  The supplementary materials, contain
plots for other values of $s$ and $m$ (performance is
similar).


\subsection{Optimal regularization behavior}\label{S:optimal}

The optimal amount of regularization depends on the population graph
and the sampling procedure.  Continuing with the simulation of
Sec.~\ref{S:estimation}, Fig.~\ref{F:optimal-frob} shows investigate
how $\eta^\ast$, the optimal value of $\eta$, depends with $s$ and
$m$.  We can see that as the sample size $m$ increases, $\eta^\ast$ converges
monotonically to $\bar \tau$.  Further, higher values of $s$ (more
expander-like graphs) correspond to higher values of $\eta^\ast$.
This behavior is consistent with \eqref{E:eta}.



\begin{figure}
    \centering
    \subfigure[Relative Frobenius error versus $\eta / \bar \tau$]{
      \makebox{\label{F:estimation-frob}\includegraphics[scale=0.5]{plots/estimation-frob-p100}}
    }
    \subfigure[Scaled optimal regularization parameter $\eta^\ast / \bar \tau$ plotted against $m$
    and $s$]{
      \makebox{\label{F:optimal-frob}\includegraphics[scale=0.5]{plots/optimal}}
    }
    %\caption{Analytical and emprical priors}
\end{figure}



%\section{Concluding remarks}
%\label{sxn:conc}

%XXX.  BRIEF CONCLUSION HERE.  SOME PHILOSOPHY AND/OR SPECIFIC STATISTICAL LOOSE ENSD THAT WE WANT PEOPLE TO FOLLOW UP ON.



\bibliographystyle{plain}
%\bibliography{refs}
\bibliography{refs,communities,mwmbib_jrnl,mwmbib_proc,mwmbib_book,mwmbib_misc,mwmbib_drft,haribib,communities}


\newpage
\appendix

%%NIPSSUB%% \subsection{Heuristic justification for the Wishart density}
\section{Supplementary Materials: Heuristic justification for the Wishart density}
\label{sxn:justification}

Here we describe a sampling procedure for $L$ which, in a very crude
sense, leads approximately to a conditional Wishart density for $p(L
\mid \mathcal{L})$.

Let $G$ be a graph with vertex set $V = \{ 1, 2, \dotsc, n \}$, edge
set $E = V \times V$ equipped with the equivalence relation
$(u,v) = (v,u)$.  Let $\omega$ be an edge weight function, and let
$\mathcal{L}$ and $\mathcal{L}_0$ be the corresponding normalized and
combinatorial Laplacians.  Let $\Delta$ be a diagonal matrix with
$\Delta(u,u) = \sum_{v} \omega(u,v)$, so that
$\mathcal{L} = \Delta^{-1/2} \mathcal{L}_0 \Delta^{-1/2}$.  Suppose
the weights are scaled such that $\sum_{(u,v) \in E} \omega(u,v) = 1$,
and suppose further that $\Delta(u,u) > 0$.
We refer to $\omega(u,v)$ as the population weight of edge $(u,v)$.

A simple model for the sample graph is as follows: we sample $m$ edges
from $E$, randomly chosen according to the population weight function.
That is, we see edges $(u_1, v_1), (u_2, v_2), \dotsc, (u_m, v_m)$,
where the edges are all drawn independently and identically such that
the probability of seeing edge $(u,v)$ is determined by $\omega$:
\[
  \prob_\omega\{ (u_1, v_1) = (u,v) \} = \omega(u,v).
\]
Notably, we will likely see duplicate edges, and not every edge with a
positive weight will get sampled.

We construct a weight function from the sampled edges, called the
sample weight function, $w$, defined such that
\[
  w(u,v) = \frac{1}{m} \sum_{i=1}^{m} 1\{ (u_i, v_i) = (u,v) \}.
\]
In turn, we construct a sample compbinatorial Laplacian, $L_0$,
defined such that
\[
  L_0(u,v)
    =
    \begin{cases}
      \sum_{w} w(u,w) &\text{when $u = v$,} \\
      -w(u,v) &\text{otherwise.}
    \end{cases}
\]
Let $D$ be a diagonal matrix such that
$D(u,u) = \sum_{v} w(u,v)$, and define $L = D^{-1/2} L_0 D^{-1/2}$.
Letting $\E_\omega$ denote expectration with respect to probability
law $\prob_\omega$, note that $\E_\omega[w(u,v)] = \omega(u,v)$,
that $\E_\omega L_0 = \mathcal{L}_0$ and that $\E_\omega D = \Delta$,
Moreover, the strong law of large numbers gaurantees that as $m$ increases,
these three quantities converge almost surely to their expectations.
Further, Slutzky's theorem gaurantees that $\sqrt{m} (L - \mathcal{L})$ and
$\sqrt{m} \Delta^{-1/2} (L_0 - \mathcal{L}_0) \Delta^{-1/2}$ converge in
distribution to the same limit.  We use this large-sample behavior to
approximate the the distrubition of $L$ by the distribution of
$\Delta^{-1/2} L_0 \Delta^{-1/2}$.  Put simply, we treat the degrees as known.

The distribution of $L_0$ is completely determined by the edge
sampling scheme laid out above.  However, the exact form for the
density involves an intractible combinatorial sum.  We appeal to a
crude approximation for the conditional density.
The approximaton works as follows:
\begin{enumerate}
\item For $i = 1, \dotsc, m$, define $x_i \in \reals^n$ such that
  \[
    x_i(u)
      =
      \begin{cases}
        +s_i &\text{when $u = u_i$,} \\
        -s_i &\text{when $u = v_i$,} \\
        0 &\text{otherwise,}
      \end{cases}
  \]
  where $s_i \in \{ -1, +1 \}$ is chosen arbitrarily.
\item Note that $L_0 = \frac{1}{m} \sum_{i=1}^m x_i x_i'$.
\item Take $s_i$ to be random, equal to $+1$ or $-1$ with probability
  $\tfrac{1}{2}$.  Approximate the distribution of $x_i$ by the
  distribution of a multivariate normal random variable, $\tilde x_i$,
  such that $x_i$ and $\tilde x_i$ have the same first and second
  moments.
\item Approximate the distribution of $L_0$ by the distribution of $\tilde L_0$, where
  \(
  \tilde L_0 = \frac{1}{m} \sum_{i=1}^m \tilde x_i \tilde x_i'.
  \)
  \item Use the asymptotic expansion above to approximate the
    distribution of $L$ by the distribution of
    $\Delta^{-1/2} \tilde L_0 \Delta^{-1/2}$.
\end{enumerate}

\noindent
The next two lemmas derive the distribution of $\tilde x_i$ and
$\tilde L_0$ in terms of $\mathcal{L}$, allowing us to get an
approximation for $p(L \mid \mathcal{L})$.

\begin{lemma}
  With $x_i$ and $\tilde x_i$ defined as above,
  \[
    \E_\omega[ x_i ] = \E_\omega[ \tilde x_i ] = 0,
  \]
  and
  \[
    \E_\omega[ x_i x_i' ] = \E_\omega [ \tilde x_i \tilde x_i' ] = \mathcal{L}_0.
  \]
\end{lemma}
\begin{proof}
The random variable $\tilde x_i$ is defined to have the same first
and second moments as $x_i$.
The first momemnt vanishes since $s_i \overset{d}{=} -s_i$ implies
that $x_i \overset{d}{=} -x_i$.  For the second moments, note that
when $u \neq v$, 
\[
  \E_\omega[x_i(u) \, x_i(v)]
  = -s_i^2 \, \prob_\omega\{ (u_i,v_i) = (u,v) \}  = -\omega(u,v)
  = \mathcal{L}_0(u,v).
\]
Likewise,
\[
  \E_\omega[\{x_i(u)\}^2]
      = \sum_{v} \prob_\omega\{ (u_i,v_i) = (u,v) \}
      = \sum_{v} \omega(u,v)
      = \mathcal{L}_0(u,u). \qedhere
\]
\end{proof}

\begin{lemma}\label{L:approx-wishart}
  The random matrix $\tilde L_0$ is distrubted as $\tfrac{1}{m}
  \mathrm{Wishart }(\mathcal{L}_0, m)$ random variable.
  This distribution is supported on the set of
  positive-semidefinite matrices with the same nullspace as $\mathcal{L}_0$.  When
  $m \geq \rank(\mathcal{L}_0)$, the distribution has a denisty on this space
  given by
  \begin{equation}\label{E:wishart-density}
   f( \tilde L_0 \mid \mathcal{L}_0, m)
      \propto
      \frac{|\tilde L_0|^{(m - \rank(\mathcal{L}) - 1)/2}
        \exp\{-\tfrac{m}{2} \Tr(\tilde L_0 \mathcal{L}_0^+) \}}
        {|\mathcal{L}_0|^{m/2}}
  \end{equation}
  where the constant of proportionality depends only on $m$ and $n$
  and where $|\cdot|$ denotes pseudodeterminant (product of nonzero
  eigenvalues).
\end{lemma}
\begin{proof}
  Since $m \tilde L$ is a sum of $m$ outer products of multivariate
  $\mathrm{Normal}(0, \mathcal{L}_0)$, it is Wishart distributed
  (by definition).
  Suppose $\rank(\mathcal{L}_0) = r$ and
  $U \in \reals^{n \times r}$ is a matrix whose columns are the
    eigenvectors of $\mathcal{L}_0$.  Note that
    $U' \tilde x_i \overset{d}{=} \mathrm{Normal}(0, U' \mathcal{L}_0 U)$,
    and that $U' \mathcal{L}_0 U$ has full rank.  Thus,
    \(
      U' \tilde L_0 U
    \)
    has a density over the space of $r \times r$ positive-semidefinite
    matrices whenever $m \geq r$.  The density of $U' \tilde L U$ is
    exactly equal to $f(\tilde L_0 \mid \mathcal{L}_0, m)$,
    defined above.
\end{proof}


Using the privious lemma, the random variable $\tilde L = \Delta^{-1/2} \tilde L_0
\Delta^{-1/2}$ has density
\[
  f(\tilde L \mid \mathcal{L}, m)
    \propto
     \frac{|\Delta^{1/2} \tilde L \Delta^{1/2}|^{(m - \rank(\mathcal{L}) - 1)/2}
        \exp\{-\tfrac{m}{2} \Tr(\Delta^{1/2} \tilde L \Delta^{1/2} \mathcal{L}_0^+) \}}
        {|\Delta^{1/2} \mathcal{L}_0 \Delta^{1/2}|^{m/2}},
\]
where we have used that $\rank(\mathcal{L}_0) = \rank(\mathcal{L})$, and
the constant of proportionality depends on $m$, $n$,
$\rank(\mathcal{L})$, and $\Delta$.  If we approximate
$| \Delta^{1/2} \tilde L \Delta^{1/2}| \approx |\Delta| |\tilde L|$
and
$\Delta^{1/2} \mathcal{L}_0^+ \Delta^{1/2} \approx \mathcal{L}^+$,
then $f$ is ``approximately''  the density of a $\tfrac{1}{m}
\mathrm{Wishart }(\mathcal{L}, m)$ random variable.  These last
approximations are necessary because $\tilde L$ and $\mathcal{L}_0$
are rank-degenerate.

\subsection{Caveats}

We do not want to overstate the validity of this argument.  The
argument makes three key approximations:
\begin{enumerate}
\item the true degree matrix $\Delta$ can be
  approximated by the observed degree matrix $D$;
\item the distribution of $x_i$, a sparse vector, is well approximated
  $\tilde x_i$, a Gaussian (dense) vector;
\item the quantities
  $| \Delta^{1/2} \tilde L \Delta^{1/2}|$
  and
  $\Delta^{1/2} \mathcal{L}_0^+ \Delta^{1/2}$
  can be replaced with
  $|\Delta| |\tilde L|$
  and $\mathcal{L}^+$.
\end{enumerate}
None of these approximations hold in general, though as argued above,
the first is plausible if $m$ is large relative to $n$.  Likewise
since $\tilde L$ and $\mathcal{L}$ are nearly full rank, the third
approximation is not too bad.  The biggest leap of faith is the second
approximation.  Notably, despite their first moments being equal,
second moments of $\tilde x_i \tilde x_i'$ and $x_i x_i'$ differ.

\section{Supplementary Materials: Additional empirical evaluation}

\begin{figure}
  \centering
  \subfigure[$m/\mu$ = 0.2]{
    \makebox{\includegraphics[scale=0.45]{plots/estimation-frob-s32-p020}}
  }
  \subfigure[$m/\mu = 2.0$]{
    \makebox{\includegraphics[scale=0.45]{plots/estimation-frob-s32-p200}}
  }
  \caption{$s = 0$ edge swaps}
\end{figure}


\begin{figure}
  \centering
  \subfigure[$m/\mu$ = 0.2]{
    \makebox{\includegraphics[scale=0.45]{plots/estimation-frob-p020}}
  }
  \subfigure[$m/\mu = 2.0$]{
    \makebox{\includegraphics[scale=0.45]{plots/estimation-frob-p200}}
  }
  \caption{$s = 4$ edge swaps}
\end{figure}

\begin{figure}
  \centering
  \subfigure[$m/\mu$ = 0.2]{
    \makebox{\includegraphics[scale=0.45]{plots/estimation-frob-s32-p020}}
  }
  \subfigure[$m/\mu = 2.0$]{
    \makebox{\includegraphics[scale=0.45]{plots/estimation-frob-s32-p200}}
  }
  \caption{$s = 32$ edge swaps}
\end{figure}

\begin{figure}
  \centering
  \subfigure[$m/\mu$ = 0.2]{
    \makebox{\includegraphics[scale=0.45]{plots/estimation-spec-s32-p020}}
  }
  \subfigure[$m/\mu = 2.0$]{
    \makebox{\includegraphics[scale=0.45]{plots/estimation-spec-s32-p200}}
  }
  \caption{$s = 0$ edge swaps}
\end{figure}


\begin{figure}
  \centering
  \subfigure[$m/\mu$ = 0.2]{
    \makebox{\includegraphics[scale=0.45]{plots/estimation-spec-p020}}
  }
  \subfigure[$m/\mu = 2.0$]{
    \makebox{\includegraphics[scale=0.45]{plots/estimation-spec-p200}}
  }
  \caption{$s = 4$ edge swaps}
\end{figure}

\begin{figure}
  \centering
  \subfigure[$m/\mu$ = 0.2]{
    \makebox{\includegraphics[scale=0.45]{plots/estimation-spec-s32-p020}}
  }
  \subfigure[$m/\mu = 2.0$]{
    \makebox{\includegraphics[scale=0.45]{plots/estimation-spec-s32-p200}}
  }
  \caption{$s = 32$ edge swaps}
\end{figure}



\end{document}
