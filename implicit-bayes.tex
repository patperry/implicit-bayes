\documentclass{article}
\RequirePackage[OT1]{fontenc}
\RequirePackage{fullpage}
\RequirePackage{amsbsy,amsmath,amssymb,amsthm}
\RequirePackage{natbib}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\trans}{\mathrm{T}}
\DeclareMathOperator*{\Tr}{Tr}
\DeclareMathOperator*{\diag}{diag}
\newcommand{\Normal}[1][]{\mathcal{N}_{#1}}
\newcommand{\Wishart}[1][]{\mathcal{W}_{#1}}
\DeclareMathOperator*{\argmin}{argmin}

\newcommand{\prob}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}

\begin{document}

\title{
  Regularized Eigenvector Estimation
}
\author{
  Patrick O.\ Perry
  \and
  Michael W.\ Mahoney
}
\date{
  \today
}
\maketitle

\begin{abstract}
\end{abstract}

\section{Graph partitioning}
\label{S:introduction}

A weighted symmetric graph, $G$ is defined by a vertex set $V_G$, and a
weight function $w_G : V_G \times V_G \to \reals_+$, where $w_G$ is symmetric
in its arguments ($w_G(u,v) = w_G(v,u)$).  The cardinality of $V_G$,
denoted $|V_G|$, is finite.

A length-$l$ path in $G$, denoted $\gamma$, is defined by a finite sequence of
vertices $\gamma(1), \gamma(2), \ldots, \gamma(l)$, where $\gamma(i) \in V_G$; given
such a path, $\gamma(1)$ is called the start of $\gamma$ and 
$\gamma(l)$ is called terminus of $\gamma$.  With respect to
the graph, $G$, vertices $u$ and $v$ are said to be $l$-connected if there
exists a length-$l$ path $\gamma$ such that $\gamma(1) = u$,
$\gamma(l) = v$, and $w(\gamma(i), \gamma(i+1)) > 0$ for $i = 1,
\ldots, l-1$.  Vertices $u$ and $v$ are said to be connected if they
are $l$-connected for some positive $l$.  Connectivity defines an
equivalence relation on the vertices of a graph.  The equivalence
classes are called the connected components of $G$; they define a
natural partition of the vertices.

\subsection{Spectral clustering}

Given a weighted symmetric graph, $G$, one can construct a positive
semidefinite matrix, $L_G \in \reals^{V_G \times V_G}$, called the combinatorial Laplacian of $G$:
\[
  L_G(u,v)
  =
  \begin{cases}
    - w_G(u,v) & \text{when $u \neq v$,} \\
    d_G(v) - w_G(v,v) & \text{otherwise,}
  \end{cases}
\]
where $d_G(v) = \sum_{v'} w_G(v,v')$ is called the degree of $v$.
By construction, $L_G$ is positive semidefinite.  Also, the all ones
vector is an eigenvector of $L_G$ with eigenvalue zero, i.e. $L_G \, 1_{V_G}
= 0$, where $1_{V_G}(v) = 1$ for all $v$; we call $1_{V_G}$ the
trivial eigenvector of $L_G$.  If $C \subseteq V_G$ is a connected component of
$G$ and $1_C(v) = 1\{ v \in C \}$, then $L_G \, 1_C = 0$.  One can
show that the multiplicity of the zero eigenvalue is equal to the
number of connected components in $G$.

When $G$ has exactly two connected components, say $C$ and $\bar C$,
the combinatorial Laplacian $L_G$ has two orthogonal eigenvectors
with eigenvalue $0$: the trivial eigenvector and
$c \cdot 1_C - c^{-1} \cdot 1_{\bar C}$, where
$c = \sqrt{|\bar C| / |C|}$.  We can recover the vertex partiton
$V_G = C \sqcup \bar C$ by ``cutting'' along the first nontrivial
eigenvector of $L_G$: if $L_G x = 0$ and $1' x = 0$, then up to a sign
change in $x$, $C = \{ v : x(v) > 0 \}$ and
$\bar C = \{ v : x(v) < 0 \}$.

The idea of spectral clustering is to partition the vertices of a
weighted graph by using the nontrivial eigenvectors of a Laplacian
(either the combinatorial Laplacian, which we have defined above, or a
degree-normalized version).  For example, to partition the vertices
into two sets, one can take $x$, the first nontrivial eigenvector of
$L_G$, and for a given threshold $K$, define the two sets of the partition as
\begin{align*}
  C(x,K)      &= \{ i : x_i >= K \}, \quad \text{and} \\
  \bar C(x,K) &= \{ i : x_i < K \}.
\end{align*}
Note that this procedure defines a partion even when the vertices of
$G$ are all connected.  Cheeger's inequality gaurantees that for an
appropriate choice of $K$, the partition is well-behaved, according to a
measure called ``conductance.''


\subsection{Diffusion clustering}

An alternative solution to the vertex partitioning problem looks at the
behaviors of certain random walks on the vertices of a graph. A (Markov)
random walk on the vertices of the graph and is defined by a
transition matrix, $M \in \reals^{V_G \times V_G}$ satisfying
$M(u,v) \geq 0$ for all $u,v$ and $\sum_v M(u,v) = 1$ for all
$u$.  The random walk starts at an initial vertex, $w(0)$; at time
$t$, given that the walk is at vertex $w(t) = u$, the probabiliity that
the walk transitions to vertex $v$ is
$\prob\{w(t+1) = v \mid w(t) = u\} = M(u, v)$.

Three possible choices are edge-weighted random walk, PageRank surfer, and heat diffusion.  The ``bottlenecks'' in the random walks reveal structure in the graph.  Given a diffusion operator, $M$, we can find a partition of the nodes by taking an initial vector $x_0$ with positive entries, choosing time horizon, $t$, and a threshold, $K$.  The first set of partition is $C(M^t x_0, K)$; the other set is the complement.  If we let $t\to \infty$, then the Laplacian-based partition will agree with the diffusion-based partition for many (all?) types of diffusion.  We can also get a ``local'' partition by taking $x_0$ to have support over a small set of nodes and chosen a small time-horizon~$t$.


\textit{\textbf{Issue (4).  What is the relationship between a random vector defined on the graph and a diffusion matrix?} The is a big issue lurking which we haven't discussed at all: specifically, we are essentially arguing that there is a connection between graph diffusions and covariance matrices.  The only connection, though, is that two seemingly-unrelated problems have the same solution.  Right now, that seems like an accident, and I would like to understand it better.}



\subsection{Relationship between spectral and and diffusion clustering}

The eigenvector corresponding to the smallest nontrivial eigenvalue of
the Laplacian $L$ solves an optimization problem:
\[
\begin{aligned}
  & \underset{x}{\text{minimize}}
  & & x' L x \\
  & \text{subject to}
  & & x' x = 1, \\
  & & & 1' x = 0
\end{aligned}
\]
\cite{mahoney2010implementing} show that diffusion operators arrise as
solutions to regularized, relaxed Laplacian eigenvector computations.
Specifically, they show that for certain choices of ``penalty''
function, $F$, the solution of an SDP relaxation of the Laplacian
eigenvector optimization problem is a diffusion operator.  The SDP
relaxation of the eigenvector problems considers $x$ to be a mean-zero
Gaussian random variable with covariance matrix $X$, the problem then, is to choose $X$ to minimize the expectation of $x' L x$, equivalently $\mathrm{tr}(L X)$.  The regularized relaxed problem is
\[
\begin{aligned}
  & \underset{X}{\text{minimize}}
  & & \mathrm{tr}(LX) + F(X) \\
  & \text{subject to}
  & & X \succeq 0, \\
  & & & \mathrm{tr}(X) = 1, \\
  & & & \mathrm{tr}(X J) = 0,
\end{aligned}
\]
where $J = 1 1'$.

The role of the penalty function, $F$, is to regularize the solution of the optimization problem.  Regularization can often be associated with prior assumptions on the data.  In linear regression, and $L^2$ penalty corresponds to a Gaussian prior on the coefficent vector, and the solution to the regularized regression problem is the maximium a posterior (MAP) estimator.  In the same setting, $L^1$ regularization corresponds to a Laplace prior on the coefficent vector.

Empirical experience has shown that certain diffusions are more or less appropriate for certain classes of graphs (expanders, low-dimension, etc.).  No one yet understands why this is the case.  Our hope is to shed light on the issue by investigating the role of $F$.  For each class of graphs, there is a corresponding class of prior distributions.  Each prior distributions manifest in a different penalty function, $F$.  Thus, the diffusion associate with $F$ will be most appropriate for graphs coming from the class determined by the prior distribution.


\section{Sample Laplacians}

We view spectral and diffusion clustering as procedures for estimating
features of an unobserved graph Laplacian.  In our formuation, there
is an unobserved graph, $G$ with vertex set $V_G = V$ and edge weight
function $w_G = w$.  Let $E$ be the subset of edges on $V$, with the
equivalence relation $(u,v) \sim (v,u)$.  Note that $|E| =
\binom{|V|}{2}$, regardless of $w$.  The weight function is normalized such that
$\sum_{(u,v) \in E} w(u,v) = 1$.  We observe $G_n$, a random weighted graph with
vertex set $V_{G_n} = V$ and edge weight function $w_{G_n} = w_n$.
The random graph has the property that $\E[w_n(u,v)] = w(u,v)$ for
each vertex pair $(u,v) \in E$.  We set $L$ to be the combinatorial
Laplacian of $G$, and set $L_n$ to be the combinatorial Laplacian of
$G_n$.  Our view is that spectral or diffusion clustering on $L_n$
attempts to uncover features of $L$.  Specifically, we view these
procedures as estimates of $L^{-1}$.  (Here and throughout, $A^{-1}$
denotes inverse in the space $R^V \perp 1_V$; likewise $\det A$
denotes determinant in this space.)

There are many schemes for sampling graphs, including snowball and
traceroute sampling (TODO: add refs.). We presume a simplistic
sampling scheme which, although not universally applicable, is at
least amenable to theoretical analysis.  Specifically, we suppose that
the edges we observed have been sampled independently according to the
weights specified by $w$, which may be unknown.

The data generating mechanism is as follows: $n$ edges are sampled
independently and in the same manner.  The $i$th edge, $(u_i, v_i)$ is determined by
choosing a random edge from $E$, with probability proportional to the
weight of the edge; i.e.
\[
p(u,v) = \prob\{ (u_i,v_i) = (u,v) \} \propto w(u,v) \}
\]
Since $\sum_{(u,v) \in E} w(u,v) = 1$, we have
\(
  p(u,v) = w(u,v)
\)

The sample weight of edge $(u,v) \in E$ is the number of times the edge
appeared in the $n$ sampled edges:
\[
  w_n(u,v) = \frac{1}{n} \sum_{i=1}^n 1\{ (u_i,v_i) = (u,v) \};
\]
we symmetrize $w_n$ so that $w_n(u,v) = w_n(v,u)$.
The sample degree of vertex $v$ is
\[
  d_n(v) = \frac{1}{n} \sum_{i = 1}^{n} 1\{ u_i = v \text{ or } v_i = v\} = \sum_u w_n(u,v).
\]
The sample Laplacian is
\[
  L_n = D_n - W_n,
\]
where $D_n$ is diagonal with $D_n(v,v) = d_n(v)$ and $W_n$ is
symmetric with $W_n(u,v) = w_n(u,v)$.

Note that $\E L_n = L$.  For large $n$, we approximate the
distribution of $L_n$ by a scaled Wishart
matrix.  To motivate the approximation, let $s_1, \ldots, s_n$ be IID
with $\prob\{ s_i = +1 \} = \prob\{ s_i = -1 \} = \tfrac{1}{2}$.
Intepret $s_i$ as the orientation of edge $i$.  Given an edge,
$(u,v) \in E$ with orientation, $s$, construct a vector, $x$, in $\reals^|V|$ by
\[
  x(u,v,s) = s \cdot \delta_u - s \cdot \delta_v,
\]
where $\delta_v$ is the Dirac delta.
Set $x_i = x(u_i, v_i, s_i)$ for $i = 1, \ldots, n$ and note that
\[
  n L_n = \sum_{i=1}^{n} x_i x_i'.
\]
Moreover, the random vector $x_i$ has expectation $0$ and covariance
matrix $L$.  The representation of $L_n$ in terms of
outer products is standard in the spectral graph theory litterature.
The random orientation $s_i$ assures the first moment of $x_i$ is
zero. For the second moments, with $(u,v) \in V^2$ we have
\begin{align*}
  \E[x_i(u) x_i(v)]
    &= \E\big[ - 1\big\{ (u_i, v_i)  \sim (u,v) \big\}\big] \\
    &= - \prob\big\{ (u_i,v_i) \sim (u,v) \big\} \\
    &= -w(u,v)
\end{align*}
and for $v \in V$,
\begin{align*}
  \E[x_i(v)^2]
    &= \E\Big[ \sum_{u \in V} 1\big\{(u_i,v_i) \sim (u,v) \Big] \\
    &= \sum_{u \in V} w(u,v).
\end{align*}

Since $L_n$ is an average of IID objects, the Central Limit Theorem gaurantees
that the elements of $\sqrt{n} (L_n - L)$ converge in distribution to a multivariate
normal.  The Lindeberg-Feller proof of this result approximates the
distribution of the elements of $x_i x_i'$ by a multivariate normal
distribution.  A more accurate distributional approximation replaces $x_i$ by a
multivariate normal, $\tilde x_i$, with the same first and second
moments, so that $x_i x_i'$ gets approximated by $\tilde x_i \tilde
x_i'$, a Wishart matrix.
 Vector $x_i$ is random with mean $0$ and covariance matrix
$L$.  Thus, $x_i x_i'$ is approximated by a central
Wishart with shape $L$ and scale $1$; the scaled sample
Laplacian $n L_n$ is then approximated by Wishart with the same shape
but scale $n$ instead, i.e. $L_n$ is approximately $\mathcal{W}(L, n)$.

\section{Connection to covariance estimation}
\label{S:connection-to-covariance}

The relaxed eigenvector problems appears related to a standard covariance estimation problem.


Let $x_1, \ldots, x_n$ be i.i.d., drawn from $\mathrm{Normal}(0, \Sigma)$
for unknown $\Sigma$.  Matrix $\Sigma^{-1}$, is called the precision
matrix.  Set $\tau = \mathrm{Tr}(\Sigma^{-1})$ and
$\Phi = \tau^{-1} \Sigma^{-1}$.  Say we have prior information that the
eigenvectors of $\Phi$ are uniformly (Haar) distributed, and that the
eigenvalues are Dirichlet distributed with parameter vector
$(\alpha, \ldots, \alpha)$ for some $\alpha > 0$.  The prior density
of $\Phi$ over the space of positive-definite matrices given by
\[
    \pi(\Phi) \propto |\Phi|^{\alpha-1}
\]
(the proportionality constant depends only on $\alpha$ and $p$).
Assume a uniform prior for $\tau$.
Define
\[
    A = \sum_{i=1}^n x_i x_i^T,
\]
the unnormalized sample covariance matrix.  Given $\Phi$, matrix $A$
follows a Wishart distribution with shape $\Sigma$ and scale $n$.  Its
density is
\[
    f(A | \Phi, \tau)
        \propto
        \exp\{
            -\tfrac{\tau}{2} A \cdot \Phi
            + \tfrac{n}{2} \log | \Phi| + \tfrac{np}{2} \log \tau
        \}
\]
(here, the proportionality constant depends only on $A$ and $n$).  The
posterior distribution for $\Phi$ and $\tau$ is the proportional to the
product of the prior and the likelihood:
\[
    \pi(\Phi, \tau | A)
        \propto
        \exp\{
            -\tfrac{\tau}{2} A \cdot \Phi
            + \tfrac{n}{2} \log | \Phi| + \tfrac{np}{2} \log \tau
            +
            (\alpha - 1)
            \log |\Phi|
        \}
\]
Collecting logs, taking logs and multiplying by $-2$, the maximum a
posteriori (MAP) estimate of $\Phi$ and $\tau$ is the solution to the
optimization problem:
\begin{align*}
    \min \qquad &\tau A \cdot \Phi - (n + 2 (\alpha - 1)) \log |\Phi| + np \log \tau \\
    \text{s.t.} \qquad & I \cdot \Phi = 1 \\
    \phantom{\text{s.t.}} \qquad & \Phi \succeq 0 \\
    \phantom{\text{s.t.}} \qquad & \tau \geq 0
\end{align*}

The solution is
\[
    \Phi = - [\tfrac{1}{n + (2\alpha - 1)}(\lambda I - \tau A)]^{-1}
\]

\textit{\textbf{Issue (1). How is $\Phi$ a pagerank?.}
We can show under certain model/prior assumptions that the MAP estimator of the inverse covariance matrix is a ``pagerank,'' $P$, but the ``pagerank'' matrix has negative entries, so what exactly does that mean?  You brought up the idea of replacing $P$ by, say, $P + x x'$ for suitable chosen $x$ such that $P + x x'$ is positive, performing the pagerank surfer walk with $P + x x'$, and then projecting orthogonal to $x$.  I'm not sure if this makes sense, though, because different choices of $x$ will give different answers.}


\section{Relationship between covariance estimation and graph Laplacians}

\textit{\textbf{Issue (2). This section is pretty flimsy.} The connection between graph Laplacians and sample covariance matrices is tenuous.  There is some sparse, graph-like structure in the inverse covariance matrix which might be the connection, but that's more like an adjacency matrix, not a Laplacian.}

Suppose we have a graph, $\mathcal{G}$ with $n$ vertices.  When we
say that $\mathcal{G}$ is ``low-dimensional,'' we mean we can embed the
vertices in $\mathbb{R}^d$ with $d \ll n$ such that Euclidean distance
for the embedded points agrees with geodesic distance on the graph.  When
we say that $\mathcal{G}$ is an ``expander,'' we mean that no low-dimensional
Euclidean embedding preserves the distances.

These concepts have analogues for sequences of high-dimensional,
independent and identically distributed (i.i.d.) points.  Let $p$ be
a dimension, conceptually big.  Suppose $x_1, \ldots, x_n$ is a sequence
of independent Gaussian-distributed vectors in $\mathbb{R}^d$ with population
mean zero and covariance matrix $\Sigma \in \mathbb{R}^{p \times p}$ 
(which is positive semi-definite).  One way of generating such a sequence
is the following:
\begin{enumerate}
    \item draw $Z$ in $\mathbb{R}^{n \times p}$, a random matrix whose
        entries are independent Gaussians having mean $0$ and variance $1$;
    \item compute $\Sigma^{1/2}$ such that
        $[\Sigma^{1/2}] [\Sigma^{1/2}]^T = \Sigma$ and set
        $X = Z [\Sigma^{1/2}]^{T}$;
    \item take $x_1, \ldots, x_n$ to be the rows of $X$.
\end{enumerate}
If $\Sigma$ has low rank, $d$, then $x_1, \ldots, x_n$ can be projected
into a $d$-dimensional subspace such that distances between the points
are preserved.  The subspace is spanned by the $d$ eigenvectors of
$\Sigma$ corresponding to its nonzero eigenvalues.  If $\Sigma$ is
a scalar multiple of the identity matrix, then no such low-dimensional
projection exists; for $n \geq p$, the $x$'s span $\mathbb{R}^p$ with
probability $1$, and they are not concentrated in any low-dimensional
subspace [this is a little hand-wavy].

In general the ``low-dimensionality'' and ``expanderness'' of a
high-dimensional distribution is determined by the eigenvalues of $\Sigma$,
its covariance matrix.  The projection onto the eigenvector of $\Sigma$
corresponding to eigenvalue $\lambda$ captures fraction
$\lambda / \mathrm{Tr}(\Sigma)$ of the total variance.  For low-dimensional
distributions, most of the eigenvalues are zero.  For expanders, the
eigenvalues are all about equal.  For in-between, the eigenvalues decay
slowly to zero.


\section{Other diffusions}

Other choices of the penalty function, $F$ yield different diffusions.  One way to derive priors for these distrubutions is by working backwards.

\textit{\textbf{The resulting priors depend on the data.  Does this make sense?}}


One Bayesianization of the regularized eigenvector problem involves the Wishart
distribution on positive-definite matrices.  Recall that if $\Sigma$ is
positive-semidefinite and $x_1, \ldots, x_n$ are independent
$\Normal[d](0, \Sigma)$ random vectors, then the scaled sample covariance matrix
defined by $A = \sum_{i=1}^n x_i x_i^\trans$ is said to follow the $d$-dimensional
Wishart distribution with shape parameter $\Sigma$ and scale $n$.  We denote
this by $A \sim \Wishart[d](\Sigma, n)$.  If $\Sigma$ is positive-definite and $n \geq d$, then
$A$ has density over the set of positive-definite matrices given by
\[
    f_{\Wishart}(A \mid \Sigma, n)
    =
    \frac{
        |A|^{(n-d-1)/2} \exp\{-\tfrac{1}{2} \Sigma^{-1} \bullet A \}
    }{
        2^{dn/2} \pi^{d(d-1)/4}
        |\Sigma|^{n/2}
        \prod_{j=1}^{d} \Gamma\big(\tfrac{1}{2} (n + 1 - j)\big)
    },
\]
where $\Gamma(\cdot)$ is the Gamma function.  For $d > 1$ this generalizes the
chi-squared distribution.  If $S = \tfrac{1}{n} A$ is a sample covariance matrix
with $n$ degrees of freedom, then $S$ has density over the set of
positive-definite matrices equal to
\(
    n^d \cdot f_{\Wishart}(n S \mid \Sigma, n).
\)

Recall that the regularized eigenvector problem takes the form
\begin{equation}\label{E:evec-reg}
    \argmin_{\Phi}
        S \bullet \Phi
        +
        1/\eta \cdot F(\Phi),
\end{equation}
where $F$ has support on the set 
\(
    \{ \Phi : \Tr(\Phi) = 1, \Phi \succeq 0 \}.
\)
It is convenient to think of $S$ as a sample covariance matrix with $n$ degrees
of freedom, so that $A = n S$ is a Wishart matrix.
The form of~\eqref{E:evec-reg} suggests interpreting $\Phi^{-1}$ as the
expectation of $S$, so that $\Phi$ is is a precision matrix
(the inverse of a covariance matrix).

Solving \eqref{E:evec-reg} is equivalent to solving the following:
\begin{equation*}
    \argmin_{\Phi}
        \big[
            (n S) \bullet \Phi
            -
            (n-d-1) \log |n S|
            -
            n \log |\Phi|
        \big]
        +
        \big[
            n/\eta \cdot F(\Phi)
            +
            n \log |\Phi|
        \big].
\end{equation*}
Recognize this as the MAP estimator for $\Phi$ after observing $S$, a 
sample covariance matrix with $n$ degrees of freedom and expectation
$\Phi^{-1}$; the prior density for $\Phi$ is supported on the
set $\{ \Phi : \Tr(\Phi) = 1, \Phi \succeq 0\}$, and is given by
\begin{equation}\label{E:prior-phi}
    p_{F}(\Phi | n, \eta)
        \propto
        \frac{
            \exp\{-\tfrac{n}{2\eta} F(\Phi) \}
        }{
            |\Phi|^{n/2}
        }.
\end{equation}
We consider $n$ and $\eta$ to be a hyper-parameters.  As before, when
$\int p_{F}(\Phi | n, \eta) d\Phi$ diverges, the prior is improper.


\subsection{Priors corresponding to orthogonally invariant penalties}

When $F$ is orthogonally invariant, $p_F(\Phi | n, \eta)$ depends only on the
eigenvalues of $\Phi$.  Letting $\Phi = O \Lambda O^\trans$ be the
eigendecomposition of $\Phi$, our prior belief is that $O$ is Haar-distributed over
the group of $d\times d$ orthogonal matrices.  Set
$\Lambda = \diag(\lambda_1, \ldots, \lambda_d)$.  Under orthogonal invariance,
the prior $p_F(\Phi | n, \eta)$ is completely specified by defining
$p_F(\Lambda | n, \eta)$.  In what follows, we derive the priors corresponding
to the choices for $F$ given in Section~3 of ``Implementing regularization
implicitly\ldots''.

\subsubsection*{Generalized Entropy}

On the simplex, for $F_H$ being Generalized Entropy,
\begin{align*}
    F_H(\Lambda)
        &= \Tr(\Lambda \log \Lambda) - \Tr(\Lambda) \\
        &= \log \Big[ \prod_{j=1}^d \lambda_j^{\lambda_j} \Big] - 1.
\end{align*}
Thus, from \eqref{E:prior-phi} it follows that
\[
    p_{F_H}(\Lambda | n, \eta)
        \propto
            \prod_{j=1}^d
                \exp\Big\{
                    -\tfrac{n}{2}
                    \big(
                        \tfrac{\lambda_j}{\eta}
                        +
                        1
                    \big)
                    \log \lambda_j
                \Big\}.
\]
This distribution does not follow a recognizable parametric form.


\subsubsection*{Log-determinant}

With $F_D$ being Log-determinant,
\begin{align*}
    F_D(\Lambda)
        &= -\log |\Lambda| \\
        &= -\log \Big[ \prod_{j=1}^d \lambda_j \Big].
\end{align*}
Thus,
\[
    p_{F_D}(\Lambda | n, \eta)
        \propto
            \prod_{j=1}^d
                \lambda_j^{n \cdot (\eta^{-1} - 1)/2}.
\]
When $n \cdot (\eta^{-1} - 1) > -2$, this defines a proper density, specifically
that of the Dirichlet distribution with parameter vector $\alpha 1_d$, where
$\alpha = \tfrac{n}{2} \cdot (\eta^{-1} - 1) + 1$.


\subsubsection*{Standard $k$-norm}

For the Standard $k$-norm function,
\[
    F_k(\Lambda)
        = \frac{1}{k} \Tr(\Lambda^k)
        = \frac{1}{k} \sum_{j=1}^d \lambda_j^k
\]
Thus,
\[
    p_{F_k}(\Lambda | n, \eta)
        \propto
            \exp\Big\{
                -
                \frac{n}{2k \eta}
                \sum_{j=1}^{d} \lambda_j^k
                -
                \frac{n}{2}
                \sum_{j=1}^{d} \log \lambda_j
            \Big\}.
\]
As with Generalized Entropy, this does not correspond to a recognizable
distribution.


\subsection{Discussion}

The priors in this section have modes where the eigenvalues are all equal,
so that $p_F(\Phi|n, \eta)$ is maximized when $\Phi = \tfrac{1}{d} I_d$.  This
follows since $p_F(\Phi|n, \eta)(\Phi)$ is log-concave in the eigenvalues of
$\Phi$ and has support where the eigenvalues are in the simplex.

As noted at the end of Section~\ref{S:Bayesianization}, the Bayesianization of
an optimization problem is not unique.  This makes interpreting the prior
$p_F(\cdot | n, \eta)$ difficult.  At first, it is troubling that the prior
depends on $n$, a parameter that appears nowhere in the optimization problem.
However, this has to be the case; there needs to be a parameter indicating
the ``precision'' of the data, $S$.

The Wishart is not the only possible distribution for $n S$.  Recall that the
Wishart distribution arrises when the the density of $S$ is supported on
$\{ S : S \succ 0 \}$, and its log is proportional to
\[
    n S \bullet \Phi
        - (n-d-1) \log |n S|.
\]
We could have pursued two alternatives:
\begin{enumerate}
    \item Instead of multiplying by $n$ and adding $-(n-d-1) \log |n S|$ to the
        optimization criterion, we could have added a different function of
        $|S|$.
    \item Instead of considering the density of $S$ to be supported over
        all positive-definite matrices, we could have considered it to be
        supported over a subset of the positive-definite matrices
        (e.g. valid graph Laplacians).  This would change the normalization
        constant (which depends on $\beta$), and would consequently change the
        prior for $\beta$.
\end{enumerate}

\section{Empirical evaluation}

\section{Concluding remarks}

\bibliographystyle{chicagoa}
\bibliography{refs.bib}

\end{document}
