\documentclass{article}
\RequirePackage[OT1]{fontenc}
\RequirePackage{fullpage}
\RequirePackage{amsbsy,amsmath,amssymb,amsthm}
\RequirePackage{natbib}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\trans}{\mathrm{T}}
\DeclareMathOperator*{\Tr}{Tr}
\DeclareMathOperator*{\diag}{diag}
\newcommand{\Normal}[1][]{\mathcal{N}_{#1}}
\newcommand{\Wishart}[1][]{\mathcal{W}_{#1}}
\DeclareMathOperator*{\argmin}{argmin}

\newcommand{\prob}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}

\begin{document}

\title{
  Regularized Eigenvector Estimation
}
\author{
  Patrick O.\ Perry
  \and
  Michael W.\ Mahoney
}
\date{
  \today
}
\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}
\label{S:introduction}

Given a weighted symmetric graph, $G$, we can construct a Laplacian, $L$.  The Laplacian is a linear operator on functions of the graph vertices.  The eigenvector of $L$ corresponding to the smallest nontrivial eigenvalue can be used to partition the nodes of $G$ into two sets such that the conductance of the partition is good.  This is the idea of spectral clustering.   The eigenvector corresponding
to the smallest eigenvalue solves an optimization problem:
\[
\begin{aligned}
  & \underset{x}{\text{minimize}}
  & & x' L x \\
  & \text{subject to}
  & & x' x = 1
\end{aligned}
\]
The solution vector $x$ facilitiates partitioning the vertices of the graph as
\begin{align*}
  S(x,t)      &= \{ i : x_i >= t \} \\
  \bar S(x,t) &= \{ i : x_i < t \}.
\end{align*}
Cheeger's inequality gaurantees that for appropriate choice of $t$, the partition has good conductance properties.

\textit{\textbf{Issue (3).  Is this correct?  Is there a quick intuitive explanation of what a Laplacian is?} I still don't have a good sense of what a graph Laplacian is, what it's properties are, and what the difference between a normalized and unnormalized Laplacian is.  It seems like you have a good grip on this, so maybe I just need to read up-- can you point me towards some relevant references?}

Alternatively, we can define a random walk on the graph $G$, also known as a diffusion.  Three possible choices are edge-weighted random walk, PageRank surfer, and heat diffusion.  The ``bottlenecks'' in the random walks reveal structure in the graph.  Given a diffusion operator, $M$, we can find a partition of the nodes by taking an initial vector $x_0$ with positive entries, choosing time horizon, $k$, and a threshold, $t$.  The first set of partition is $S(M^k x_0, t)$; the other set is the complement.  If we let $k\to \infty$, then the Laplacian-based partition will agree with the diffusion-based partition for many (all?) types of diffusion.  We can also get a ``local'' partition by taking $x_0$ to have support over a small set of nodes and chosen a small time-horizon~$k$.

\cite{mahoney2010implementing} show that diffusion operators arrise as solutions to regularized, relaxed Laplacian eigenvector computations.  Specifically, they show that for certain choices of ``penalty'' function, $F$, the solution of an SDP relaxation of the Laplacian eigenvector optimization problem is a diffusion operator.  The SDP relaxation of the eigenvector problems considers $x$ to be a Gaussian random variable with zero mean and covariance matrix $X$, the problem then, is to choose $X$ to minimize the expectation of $x' L x$, equivalently $\mathrm{tr}(L X)$.  The regularized relaxed problem is
\[
\begin{aligned}
  & \underset{X}{\text{minimize}}
  & & \mathrm{tr}(LX) + F(X) \\
  & \text{subject to}
  & & \mathrm{tr}(X) = 1 \\
  & & & X \succeq 0
\end{aligned}.
\]

\textit{\textbf{Issue (4).  What is the relationship between a random vector defined on the graph and a diffusion matrix?} The is a big issue lurking which we haven't discussed at all: specifically, we are essentially arguing that there is a connection between graph diffusions and covariance matrices.  The only connection, though, is that two seemingly-unrelated problems have the same solution.  Right now, that seems like an accident, and I would like to understand it better.}

The role of the penalty function, $F$, is to regularize the solution of the optimization problem.  Regularization can often be associated with prior assumptions on the data.  In linear regression, and $L^2$ penalty corresponds to a Gaussian prior on the coefficent vector, and the solution to the regularized regression problem is the maximium a posterior (MAP) estimator.  In the same setting, $L^1$ regularization corresponds to a Laplace prior on the coefficent vector.

Empirical experience has shown that certain diffusions are more or less appropriate for certain classes of graphs (expanders, low-dimension, etc.).  No one yet understands why this is the case.  Our hope is to shed light on the issue by investigating the role of $F$.  For each class of graphs, there is a corresponding class of prior distributions.  Each prior distributions manifest in a different penalty function, $F$.  Thus, the diffusion associate with $F$ will be most appropriate for graphs coming from the class determined by the prior distribution.


\section{Sample Laplacians}

To cast graph partitioning as an estimation problem, we need to take
into about the data-generating sampling mechanism.  Suppose
$G~=~(V,E)$ is an undirected graph, where $V = \{ 1, \ldots, N \}$,
and let $w : E \to \reals_+$ assign nonnegative weights to the edges
of the graph.  Set $W~=~\sum_{e \in E} w(e)$ to be the total edge
weight and set $L$ to be the combinatorial Laplacian of the graph.

In problems of practical interest, we do not observe the graph in
entirety, but merely a sample of its edges.  There are many schemes for sampling
graphs, including snowball and traceroute sampling (TODO: add refs.).
We presume a simplistic sampling scheme which, although not
universally applicable, is at least amenable to theoretical analysis.
Specifically, we suppose that the edges we observed have been sampled
independently according to the weights specified by $w$, which may be
unknown.

The data generating mechanism is as follows: $n$ edges are sampled
independently and in the same manner.  The $i$th edge, $e_i$ is determined by
choosing a random edge from $E$, with probability proportional to the
weight of the edge; for fixed edge $e$, the probability of $e_i$ being
equal to $e$ is $w(e) / W$.  The sample degree of vertex $v$ is
\[
  d_n(v) = \sum_{i = 1}^{n} 1\{ v \in e \};
\]
the sample weight of edge $e$ is
\[
  w_n(e) = \sum_{i = 1}^{n} 1\{ e_i = e \};
\]
and the sample Laplacian is
\[
  L_n = D_n - W_n,
\]
where $D_n = \diag(d_n(1), \ldots, d_n(N))$ and
$W_n(u,v) = w_n(\{u,v\})$.

For large $n$, we approximate the distribution of $L_n$ by a Wishart
matrix.  To motivate the approximation, let $s_1, \ldots, s_n$ be IID
with $\prob\{ s_i = +1 \} = \prob\{ s_i = -1 \} = \tfrac{1}{2}$.
Intepret $s_i$ as the orientation of edge $i$.  Given an edge,
$e = \{ u, v \}$ with $u \leq v$, and an
orientation, $s$, construct a vector, $x$, in $\reals^N$ by
\[
  x(e,s) = s \cdot \delta_u - s \cdot \delta_v,
\]
where $\delta_v$ is the Dirac delta.
Set $x_i = x(e_i, s_i)$ for $i = 1, \ldots, n$ and note that
\[
  L_n = \sum_{i=1}^{n} x_i x_i'.
\]
Moreover, the random vector $x_i$ has expectation $0$ and covariance
matrix $\tfrac{1}{W} L$.  The representation of $L_n$ in terms of
outer products is standard in the spectral graph theory litterature.
The random orientation $s_i$ assures the first moment of $x_i$ is
zero. For the second moments, with $u,v$ in $V$ and $u \neq v$ we have
\begin{align*}
  \E[x_i(u) x_i(v)]
    &= \E\big[ - 1\big\{ e_i = \{u,v\}\big\}\big] \\
    &= - \prob\big\{ e_i = \{u,v\}\big\} \\
    &= -\frac{w(u,v)}{W}
\end{align*}
and
\begin{align*}
  \E[x_i(u)^2]
    &= \E\Big[ \sum_{v \in V} 1\big\{e_i = \{u,v\}\big\} \Big] \\
    &= \sum_{v \in V} \frac{w(u,v)}{W}.
\end{align*}

Since $L_n$ is a sum of IID objects, the Central Limit Theorem gaurantees
that the elements of $(1/\sqrt{n}) L_n$ converge in distribution to a multivariate
normal.  The Lindeberg-Feller proof of this result approximates the
distribution of the elements of $x_i x_i'$ by a multivariate normal
distribution.  A more accurate distributional approximation replaces $x_i$ by a
multivariate normal, $\tilde x_i$, with the same first and second
moments, so that $x_i x_i'$ gets approximated by $\tilde x_i \tilde
x_i'$, a Wishart matrix.
 Vector $x_i$ is random with mean $0$ and covariance matrix
$\tfrac{1}{W} L$.  Thus, $x_i x_i'$ is approximated by a central
Wishart with shape $\tfrac{1}{W} L$ and scale $1$; the sample
Laplacian $L_n$ is then approximated by Wishart with the same shape
but scale $n$ instead, i.e. $L_n$ is approximately $\mathcal{W}(\tfrac{1}{W} L, n)$.

\section{Connection to covariance estimation}
\label{S:connection-to-covariance}

The relaxed eigenvector problems appears related to a standard covariance estimation problem.


Let $x_1, \ldots, x_n$ be i.i.d., drawn from $\mathrm{Normal}(0, \Sigma)$
for unknown $\Sigma$.  Matrix $\Sigma^{-1}$, is called the precision
matrix.  Set $\tau = \mathrm{Tr}(\Sigma^{-1})$ and
$\Phi = \tau^{-1} \Sigma^{-1}$.  Say we have prior information that the
eigenvectors of $\Phi$ are uniformly (Haar) distributed, and that the
eigenvalues are Dirichlet distributed with parameter vector
$(\alpha, \ldots, \alpha)$ for some $\alpha > 0$.  The prior density
of $\Phi$ over the space of positive-definite matrices given by
\[
    \pi(\Phi) \propto |\Phi|^{\alpha-1}
\]
(the proportionality constant depends only on $\alpha$ and $p$).
Assume a uniform prior for $\tau$.
Define
\[
    A = \sum_{i=1}^n x_i x_i^T,
\]
the unnormalized sample covariance matrix.  Given $\Phi$, matrix $A$
follows a Wishart distribution with shape $\Sigma$ and scale $n$.  Its
density is
\[
    f(A | \Phi, \tau)
        \propto
        \exp\{
            -\tfrac{\tau}{2} A \cdot \Phi
            + \tfrac{n}{2} \log | \Phi| + \tfrac{np}{2} \log \tau
        \}
\]
(here, the proportionality constant depends only on $A$ and $n$).  The
posterior distribution for $\Phi$ and $\tau$ is the proportional to the
product of the prior and the likelihood:
\[
    \pi(\Phi, \tau | A)
        \propto
        \exp\{
            -\tfrac{\tau}{2} A \cdot \Phi
            + \tfrac{n}{2} \log | \Phi| + \tfrac{np}{2} \log \tau
            +
            (\alpha - 1)
            \log |\Phi|
        \}
\]
Collecting logs, taking logs and multiplying by $-2$, the maximum a
posteriori (MAP) estimate of $\Phi$ and $\tau$ is the solution to the
optimization problem:
\begin{align*}
    \min \qquad &\tau A \cdot \Phi - (n + 2 (\alpha - 1)) \log |\Phi| + np \log \tau \\
    \text{s.t.} \qquad & I \cdot \Phi = 1 \\
    \phantom{\text{s.t.}} \qquad & \Phi \succeq 0 \\
    \phantom{\text{s.t.}} \qquad & \tau \geq 0
\end{align*}

The solution is
\[
    \Phi = - [\tfrac{1}{n + (2\alpha - 1)}(\lambda I - \tau A)]^{-1}
\]

\textit{\textbf{Issue (1). How is $\Phi$ a pagerank?.}
We can show under certain model/prior assumptions that the MAP estimator of the inverse covariance matrix is a ``pagerank,'' $P$, but the ``pagerank'' matrix has negative entries, so what exactly does that mean?  You brought up the idea of replacing $P$ by, say, $P + x x'$ for suitable chosen $x$ such that $P + x x'$ is positive, performing the pagerank surfer walk with $P + x x'$, and then projecting orthogonal to $x$.  I'm not sure if this makes sense, though, because different choices of $x$ will give different answers.}


\section{Relationship between covariance estimation and graph Laplacians}

\textit{\textbf{Issue (2). This section is pretty flimsy.} The connection between graph Laplacians and sample covariance matrices is tenuous.  There is some sparse, graph-like structure in the inverse covariance matrix which might be the connection, but that's more like an adjacency matrix, not a Laplacian.}

Suppose we have a graph, $\mathcal{G}$ with $n$ vertices.  When we
say that $\mathcal{G}$ is ``low-dimensional,'' we mean we can embed the
vertices in $\mathbb{R}^d$ with $d \ll n$ such that Euclidean distance
for the embedded points agrees with geodesic distance on the graph.  When
we say that $\mathcal{G}$ is an ``expander,'' we mean that no low-dimensional
Euclidean embedding preserves the distances.

These concepts have analogues for sequences of high-dimensional,
independent and identically distributed (i.i.d.) points.  Let $p$ be
a dimension, conceptually big.  Suppose $x_1, \ldots, x_n$ is a sequence
of independent Gaussian-distributed vectors in $\mathbb{R}^d$ with population
mean zero and covariance matrix $\Sigma \in \mathbb{R}^{p \times p}$ 
(which is positive semi-definite).  One way of generating such a sequence
is the following:
\begin{enumerate}
    \item draw $Z$ in $\mathbb{R}^{n \times p}$, a random matrix whose
        entries are independent Gaussians having mean $0$ and variance $1$;
    \item compute $\Sigma^{1/2}$ such that
        $[\Sigma^{1/2}] [\Sigma^{1/2}]^T = \Sigma$ and set
        $X = Z [\Sigma^{1/2}]^{T}$;
    \item take $x_1, \ldots, x_n$ to be the rows of $X$.
\end{enumerate}
If $\Sigma$ has low rank, $d$, then $x_1, \ldots, x_n$ can be projected
into a $d$-dimensional subspace such that distances between the points
are preserved.  The subspace is spanned by the $d$ eigenvectors of
$\Sigma$ corresponding to its nonzero eigenvalues.  If $\Sigma$ is
a scalar multiple of the identity matrix, then no such low-dimensional
projection exists; for $n \geq p$, the $x$'s span $\mathbb{R}^p$ with
probability $1$, and they are not concentrated in any low-dimensional
subspace [this is a little hand-wavy].

In general the ``low-dimensionality'' and ``expanderness'' of a
high-dimensional distribution is determined by the eigenvalues of $\Sigma$,
its covariance matrix.  The projection onto the eigenvector of $\Sigma$
corresponding to eigenvalue $\lambda$ captures fraction
$\lambda / \mathrm{Tr}(\Sigma)$ of the total variance.  For low-dimensional
distributions, most of the eigenvalues are zero.  For expanders, the
eigenvalues are all about equal.  For in-between, the eigenvalues decay
slowly to zero.


\section{Other diffusions}

Other choices of the penalty function, $F$ yield different diffusions.  One way to derive priors for these distrubutions is by working backwards.

\textit{\textbf{The resulting priors depend on the data.  Does this make sense?}}


One Bayesianization of the regularized eigenvector problem involves the Wishart
distribution on positive-definite matrices.  Recall that if $\Sigma$ is
positive-semidefinite and $x_1, \ldots, x_n$ are independent
$\Normal[d](0, \Sigma)$ random vectors, then the scaled sample covariance matrix
defined by $A = \sum_{i=1}^n x_i x_i^\trans$ is said to follow the $d$-dimensional
Wishart distribution with shape parameter $\Sigma$ and scale $n$.  We denote
this by $A \sim \Wishart[d](\Sigma, n)$.  If $\Sigma$ is positive-definite and $n \geq d$, then
$A$ has density over the set of positive-definite matrices given by
\[
    f_{\Wishart}(A \mid \Sigma, n)
    =
    \frac{
        |A|^{(n-d-1)/2} \exp\{-\tfrac{1}{2} \Sigma^{-1} \bullet A \}
    }{
        2^{dn/2} \pi^{d(d-1)/4}
        |\Sigma|^{n/2}
        \prod_{j=1}^{d} \Gamma\big(\tfrac{1}{2} (n + 1 - j)\big)
    },
\]
where $\Gamma(\cdot)$ is the Gamma function.  For $d > 1$ this generalizes the
chi-squared distribution.  If $S = \tfrac{1}{n} A$ is a sample covariance matrix
with $n$ degrees of freedom, then $S$ has density over the set of
positive-definite matrices equal to
\(
    n^d \cdot f_{\Wishart}(n S \mid \Sigma, n).
\)

Recall that the regularized eigenvector problem takes the form
\begin{equation}\label{E:evec-reg}
    \argmin_{\Phi}
        S \bullet \Phi
        +
        1/\eta \cdot F(\Phi),
\end{equation}
where $F$ has support on the set 
\(
    \{ \Phi : \Tr(\Phi) = 1, \Phi \succeq 0 \}.
\)
It is convenient to think of $S$ as a sample covariance matrix with $n$ degrees
of freedom, so that $A = n S$ is a Wishart matrix.
The form of~\eqref{E:evec-reg} suggests interpreting $\Phi^{-1}$ as the
expectation of $S$, so that $\Phi$ is is a precision matrix
(the inverse of a covariance matrix).

Solving \eqref{E:evec-reg} is equivalent to solving the following:
\begin{equation*}
    \argmin_{\Phi}
        \big[
            (n S) \bullet \Phi
            -
            (n-d-1) \log |n S|
            -
            n \log |\Phi|
        \big]
        +
        \big[
            n/\eta \cdot F(\Phi)
            +
            n \log |\Phi|
        \big].
\end{equation*}
Recognize this as the MAP estimator for $\Phi$ after observing $S$, a 
sample covariance matrix with $n$ degrees of freedom and expectation
$\Phi^{-1}$; the prior density for $\Phi$ is supported on the
set $\{ \Phi : \Tr(\Phi) = 1, \Phi \succeq 0\}$, and is given by
\begin{equation}\label{E:prior-phi}
    p_{F}(\Phi | n, \eta)
        \propto
        \frac{
            \exp\{-\tfrac{n}{2\eta} F(\Phi) \}
        }{
            |\Phi|^{n/2}
        }.
\end{equation}
We consider $n$ and $\eta$ to be a hyper-parameters.  As before, when
$\int p_{F}(\Phi | n, \eta) d\Phi$ diverges, the prior is improper.


\subsection{Priors corresponding to orthogonally invariant penalties}

When $F$ is orthogonally invariant, $p_F(\Phi | n, \eta)$ depends only on the
eigenvalues of $\Phi$.  Letting $\Phi = O \Lambda O^\trans$ be the
eigendecomposition of $\Phi$, our prior belief is that $O$ is Haar-distributed over
the group of $d\times d$ orthogonal matrices.  Set
$\Lambda = \diag(\lambda_1, \ldots, \lambda_d)$.  Under orthogonal invariance,
the prior $p_F(\Phi | n, \eta)$ is completely specified by defining
$p_F(\Lambda | n, \eta)$.  In what follows, we derive the priors corresponding
to the choices for $F$ given in Section~3 of ``Implementing regularization
implicitly\ldots''.

\subsubsection*{Generalized Entropy}

On the simplex, for $F_H$ being Generalized Entropy,
\begin{align*}
    F_H(\Lambda)
        &= \Tr(\Lambda \log \Lambda) - \Tr(\Lambda) \\
        &= \log \Big[ \prod_{j=1}^d \lambda_j^{\lambda_j} \Big] - 1.
\end{align*}
Thus, from \eqref{E:prior-phi} it follows that
\[
    p_{F_H}(\Lambda | n, \eta)
        \propto
            \prod_{j=1}^d
                \exp\Big\{
                    -\tfrac{n}{2}
                    \big(
                        \tfrac{\lambda_j}{\eta}
                        +
                        1
                    \big)
                    \log \lambda_j
                \Big\}.
\]
This distribution does not follow a recognizable parametric form.


\subsubsection*{Log-determinant}

With $F_D$ being Log-determinant,
\begin{align*}
    F_D(\Lambda)
        &= -\log |\Lambda| \\
        &= -\log \Big[ \prod_{j=1}^d \lambda_j \Big].
\end{align*}
Thus,
\[
    p_{F_D}(\Lambda | n, \eta)
        \propto
            \prod_{j=1}^d
                \lambda_j^{n \cdot (\eta^{-1} - 1)/2}.
\]
When $n \cdot (\eta^{-1} - 1) > -2$, this defines a proper density, specifically
that of the Dirichlet distribution with parameter vector $\alpha 1_d$, where
$\alpha = \tfrac{n}{2} \cdot (\eta^{-1} - 1) + 1$.


\subsubsection*{Standard $k$-norm}

For the Standard $k$-norm function,
\[
    F_k(\Lambda)
        = \frac{1}{k} \Tr(\Lambda^k)
        = \frac{1}{k} \sum_{j=1}^d \lambda_j^k
\]
Thus,
\[
    p_{F_k}(\Lambda | n, \eta)
        \propto
            \exp\Big\{
                -
                \frac{n}{2k \eta}
                \sum_{j=1}^{d} \lambda_j^k
                -
                \frac{n}{2}
                \sum_{j=1}^{d} \log \lambda_j
            \Big\}.
\]
As with Generalized Entropy, this does not correspond to a recognizable
distribution.


\subsection{Discussion}

The priors in this section have modes where the eigenvalues are all equal,
so that $p_F(\Phi|n, \eta)$ is maximized when $\Phi = \tfrac{1}{d} I_d$.  This
follows since $p_F(\Phi|n, \eta)(\Phi)$ is log-concave in the eigenvalues of
$\Phi$ and has support where the eigenvalues are in the simplex.

As noted at the end of Section~\ref{S:Bayesianization}, the Bayesianization of
an optimization problem is not unique.  This makes interpreting the prior
$p_F(\cdot | n, \eta)$ difficult.  At first, it is troubling that the prior
depends on $n$, a parameter that appears nowhere in the optimization problem.
However, this has to be the case; there needs to be a parameter indicating
the ``precision'' of the data, $S$.

The Wishart is not the only possible distribution for $n S$.  Recall that the
Wishart distribution arrises when the the density of $S$ is supported on
$\{ S : S \succ 0 \}$, and its log is proportional to
\[
    n S \bullet \Phi
        - (n-d-1) \log |n S|.
\]
We could have pursued two alternatives:
\begin{enumerate}
    \item Instead of multiplying by $n$ and adding $-(n-d-1) \log |n S|$ to the
        optimization criterion, we could have added a different function of
        $|S|$.
    \item Instead of considering the density of $S$ to be supported over
        all positive-definite matrices, we could have considered it to be
        supported over a subset of the positive-definite matrices
        (e.g. valid graph Laplacians).  This would change the normalization
        constant (which depends on $\beta$), and would consequently change the
        prior for $\beta$.
\end{enumerate}

\section{Empirical evaluation}

\section{Concluding remarks}

\bibliographystyle{chicagoa}
\bibliography{refs.bib}

\end{document}
